{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lheinzel/UAVHRBuildingDetection/blob/SSD_MobNet_320x320_Augmenter/Tensorflow/workspace/training_SSD-MobnetV2_320x320_ExtPreproc/HR_BuildingDetector_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzx9J_TlztU2"
      },
      "source": [
        "# Setup Project Parameters and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "iQUQ5KGU1dxB"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsUgUFXIz23i"
      },
      "source": [
        "Project parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7wQ7GzPHz5WH"
      },
      "outputs": [],
      "source": [
        "CUSTOM_MODEL_NAME = 'HRDetection_MobNetV2'\n",
        "PRETRAINED_MODEL_NAME = 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8'\n",
        "PRETRAINED_MODEL_URL = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz'\n",
        "GITHUB_REPO_URL = 'https://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@github.com/lheinzel/UAVHRBuildingDetection.git'\n",
        "INPUT_DIMS = [320, 320]\n",
        "LABEL_MAP_NAME = 'labelmap.pbtxt'\n",
        "NOF_CLASSES = 1\n",
        "GIT_BRANCH = \"SSD_MobNet_320x320_Augmenter\"\n",
        "NOF_STEPS = 20000\n",
        "BATCH_SIZE = 16\n",
        "SIZE_TEST_VALID_SET = 1350\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaAB_EZw04EW"
      },
      "source": [
        "Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6WQG9nQK05hv"
      },
      "outputs": [],
      "source": [
        "paths = {\"github_repo\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\"),\n",
        "         \"workspace\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\"),\n",
        "         \"annotations\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"annotations\"),\n",
        "         \"images\": os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"images\"),\n",
        "         \"training\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"training_SSD-MobnetV2_320x320_ExtPreproc\"),\n",
        "         \"scripts_pre\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"scripts\",\"preprocessing\"),\n",
        "         \"pretrained_models\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"pretrained_model\"),\n",
        "         \"custom_models\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"training_SSD-MobnetV2_320x320_ExtPreproc\", \"models\"),\n",
        "         \"obj_detection_api\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\", \"models\", \"research\", \"object_detection\"),\n",
        "         \"custom_model_dir\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"training_SSD-MobnetV2_320x320_ExtPreproc\", \"models\",CUSTOM_MODEL_NAME),\n",
        "         \"custom_model_config\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"training_SSD-MobnetV2_320x320_ExtPreproc\", \"models\",CUSTOM_MODEL_NAME, \"pipeline.config\"),\n",
        "         \"model_export_dir\" : os.path.join(\"//content\",\"UAVHRBuildingDetection\",\"Tensorflow\",\"workspace\",\"training_SSD-MobnetV2_320x320_ExtPreproc\",\"exported_models\"),\n",
        "         \"gdrive_data\" : os.path.join(\"//content\", \"drive\", \"MyDrive\", \"UAVHRBuildingDetection_Dataset\", \"SSD-MobNet_320x320\")\n",
        "         }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CecPFTsQfH3"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "9RChn368YGb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M63hV2z3YJ3L",
        "outputId": "6fa76f7d-a61e-43cf-d6ab-33a53e08c6c5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1ZaBbmBTJvC"
      },
      "source": [
        "Clone the project repository from github"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIN-rdTzTO6e",
        "outputId": "9a75e15e-68ed-4bde-b9c1-3beccbfb6a22"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'UAVHRBuildingDetection'...\n",
            "remote: Enumerating objects: 6321, done.\u001b[K\n",
            "remote: Counting objects: 100% (231/231), done.\u001b[K\n",
            "remote: Compressing objects: 100% (153/153), done.\u001b[K\n",
            "remote: Total 6321 (delta 81), reused 212 (delta 71), pack-reused 6090\u001b[K\n",
            "Receiving objects: 100% (6321/6321), 4.18 GiB | 17.18 MiB/s, done.\n",
            "Resolving deltas: 100% (649/649), done.\n",
            "Checking out files: 100% (76/76), done.\n",
            "[Errno 2] No such file or directory: 'paths[github_repo]'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists(paths[\"github_repo\"]):\n",
        "    !git clone -b {GIT_BRANCH} --single-branch {GITHUB_REPO_URL}\n",
        "\n",
        "%cd paths[\"github_repo\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE9tdhlZTRK1"
      },
      "source": [
        "Install the object detection api and all other necessary things on the runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JD8AsOp3QoNR",
        "outputId": "c21e5978-c80c-4ef9-9fae-9e44e86d0271"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow==2.8\n",
            "  Downloading https://us-python.pkg.dev/colab-wheels/public/tensorflow/tensorflow-2.8.0%2Bzzzcolab20220506162203-cp37-cp37m-linux_x86_64.whl (668.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 668.3 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.21.6)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.17.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.2.0)\n",
            "Collecting keras<2.9,>=2.8.0rc0\n",
            "  Downloading keras-2.8.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 37.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.27.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.12)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.49.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (57.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.14.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.6.3)\n",
            "Collecting tf-estimator-nightly==2.8.0.dev2021122109\n",
            "  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n",
            "\u001b[K     |████████████████████████████████| 462 kB 71.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.1.2)\n",
            "Collecting tensorboard<2.9,>=2.8\n",
            "  Downloading tensorboard-2.8.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (2.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (1.3.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (14.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (4.1.1)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.8) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow==2.8) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow==2.8) (1.5.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.0.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (1.8.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow==2.8) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.9)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow==2.8) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow==2.8) (3.2.1)\n",
            "Installing collected packages: tf-estimator-nightly, tensorboard, keras, tensorflow\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.9.1\n",
            "    Uninstalling tensorboard-2.9.1:\n",
            "      Successfully uninstalled tensorboard-2.9.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "Successfully installed keras-2.8.0 tensorboard-2.8.0 tensorflow-2.8.0+zzzcolab20220506162203 tf-estimator-nightly-2.8.0.dev2021122109\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following packages will be REMOVED:\n",
            "  libcudnn8-dev\n",
            "The following held packages will be changed:\n",
            "  libcudnn8\n",
            "The following packages will be DOWNGRADED:\n",
            "  libcudnn8\n",
            "0 upgraded, 0 newly installed, 1 downgraded, 1 to remove and 25 not upgraded.\n",
            "Need to get 430 MB of archives.\n",
            "After this operation, 1,392 MB disk space will be freed.\n",
            "Get:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  libcudnn8 8.1.0.77-1+cuda11.2 [430 MB]\n",
            "Fetched 430 MB in 9s (47.6 MB/s)\n",
            "(Reading database ... 123942 files and directories currently installed.)\n",
            "Removing libcudnn8-dev (8.1.1.33-1+cuda11.2) ...\n",
            "update-alternatives: removing manually selected alternative - switching libcudnn to auto mode\n",
            "\u001b[1mdpkg:\u001b[0m \u001b[1;33mwarning:\u001b[0m downgrading libcudnn8 from 8.1.1.33-1+cuda11.2 to 8.1.0.77-1+cuda11.2\n",
            "(Reading database ... 123919 files and directories currently installed.)\n",
            "Preparing to unpack .../libcudnn8_8.1.0.77-1+cuda11.2_amd64.deb ...\n",
            "Unpacking libcudnn8 (8.1.0.77-1+cuda11.2) over (8.1.1.33-1+cuda11.2) ...\n",
            "Setting up libcudnn8 (8.1.0.77-1+cuda11.2) ...\n",
            "2022-10-22 20:51:05.649336: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "tf.Tensor(-987.7788, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "%cd {paths[\"github_repo\"]}\n",
        "\n",
        "# install/upgrade tensorflow\n",
        "#!pip install --ignore-installed --upgrade tensorflow==2.5.0\n",
        "!pip install tensorflow==2.8\n",
        "!apt install --allow-change-held-packages libcudnn8=8.1.0.77-1+cuda11.2\n",
        "\n",
        "# verify installation\n",
        "!python -c \"import tensorflow as tf; print(tf.reduce_sum(tf.random.normal([1000, 1000])))\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3eUZ2lN6xJrr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "945de786-d100-4478-e97e-5a3c51b06845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/UAVHRBuildingDetection/Tensorflow’: File exists\n",
            "/content/UAVHRBuildingDetection/Tensorflow\n",
            "Cloning into 'models'...\n",
            "remote: Enumerating objects: 78241, done.\u001b[K\n",
            "remote: Counting objects: 100% (44/44), done.\u001b[K\n",
            "remote: Compressing objects: 100% (37/37), done.\u001b[K\n",
            "remote: Total 78241 (delta 19), reused 18 (delta 7), pack-reused 78197\u001b[K\n",
            "Receiving objects: 100% (78241/78241), 593.49 MiB | 17.17 MiB/s, done.\n",
            "Resolving deltas: 100% (55625/55625), done.\n"
          ]
        }
      ],
      "source": [
        "# download model zoo if not present\n",
        "if not os.path.exists(r\"/content/UAVHRBuildingDetection/Tensorflow/models\"):\n",
        "  !mkdir /content/UAVHRBuildingDetection/Tensorflow\n",
        "  %cd /content/UAVHRBuildingDetection/Tensorflow\n",
        "  !git clone https://github.com/tensorflow/models.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BjKgzSLxMW9",
        "outputId": "33ef27ad-3cc6-4f14-efbb-1e51ce8b65e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection/Tensorflow/models/research\n"
          ]
        }
      ],
      "source": [
        "# protobuf compilation\n",
        "%cd /content/UAVHRBuildingDetection/Tensorflow/models/research/\n",
        "!protoc object_detection/protos/*.proto --python_out=.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiyKej9ZxQO_",
        "outputId": "c68d48bc-02a5-4142-e064-b74312e15b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection\n",
            "Cloning into 'cocoapi'...\n",
            "remote: Enumerating objects: 975, done.\u001b[K\n",
            "remote: Total 975 (delta 0), reused 0 (delta 0), pack-reused 975\u001b[K\n",
            "Receiving objects: 100% (975/975), 11.72 MiB | 16.31 MiB/s, done.\n",
            "Resolving deltas: 100% (576/576), done.\n",
            "/content/UAVHRBuildingDetection/cocoapi/PythonAPI\n",
            "python setup.py build_ext --inplace\n",
            "running build_ext\n",
            "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
            "/usr/local/lib/python3.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /content/UAVHRBuildingDetection/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
            "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
            "building 'pycocotools._mask' extension\n",
            "creating build\n",
            "creating build/common\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "creating build/temp.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c ../common/maskApi.c -o build/temp.linux-x86_64-3.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleDecode\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; v=!v; }}\n",
            "       \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:46:49:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "       for( k=0; k<R[i].cnts[j]; k++ ) *(M++)=v; \u001b[01;36m\u001b[Kv\u001b[m\u001b[K=!v; }}\n",
            "                                                 \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrPoly\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); x[k]=x[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:166:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) x[j]=(int)(scale*xy[j*2+0]+.5); \u001b[01;36m\u001b[Kx\u001b[m\u001b[K[k]=x[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kfor\u001b[m\u001b[K(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); y[k]=y[0];\n",
            "   \u001b[01;35m\u001b[K^~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:167:54:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kfor\u001b[m\u001b[K’\n",
            "   for(j=0; j<k; j++) y[j]=(int)(scale*xy[j*2+1]+.5); \u001b[01;36m\u001b[Ky\u001b[m\u001b[K[k]=y[0];\n",
            "                                                      \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:7:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "       \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(more) c |= 0x20; c+=48; s[p++]=c;\n",
            "       \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:212:27:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "       if(more) c |= 0x20; \u001b[01;36m\u001b[Kc\u001b[m\u001b[K+=48; s[p++]=c;\n",
            "                           \u001b[01;36m\u001b[K^\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleFrString\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:3:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "   \u001b[01;35m\u001b[Kwhile\u001b[m\u001b[K( s[m] ) m++; cnts=malloc(sizeof(uint)*m); m=0;\n",
            "   \u001b[01;35m\u001b[K^~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:220:22:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kwhile\u001b[m\u001b[K’\n",
            "   while( s[m] ) m++; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K=malloc(sizeof(uint)*m); m=0;\n",
            "                      \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:5:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kthis ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’ clause does not guard... [\u001b[01;35m\u001b[K-Wmisleading-indentation\u001b[m\u001b[K]\n",
            "     \u001b[01;35m\u001b[Kif\u001b[m\u001b[K(m>2) x+=(long) cnts[m-2]; cnts[m++]=(uint) x;\n",
            "     \u001b[01;35m\u001b[K^~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:228:34:\u001b[m\u001b[K \u001b[01;36m\u001b[Knote: \u001b[m\u001b[K...this statement, but the latter is misleadingly indented as if it were guarded by the ‘\u001b[01m\u001b[Kif\u001b[m\u001b[K’\n",
            "     if(m>2) x+=(long) cnts[m-2]; \u001b[01;36m\u001b[Kcnts\u001b[m\u001b[K[m++]=(uint) x;\n",
            "                                  \u001b[01;36m\u001b[K^~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[K../common/maskApi.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[KrleToBbox\u001b[m\u001b[K’:\n",
            "\u001b[01m\u001b[K../common/maskApi.c:141:31:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kxp\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n",
            "       if(j%2==0) xp=x; else if\u001b[01;35m\u001b[K(\u001b[m\u001b[Kxp<x) { ys=0; ye=h-1; }\n",
            "                               \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I../common -I/usr/include/python3.7m -c pycocotools/_mask.c -o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
            "creating build/lib.linux-x86_64-3.7\n",
            "creating build/lib.linux-x86_64-3.7/pycocotools\n",
            "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 -Wl,-Bsymbolic-functions -g -fwrapv -O2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/../common/maskApi.o build/temp.linux-x86_64-3.7/pycocotools/_mask.o -o build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so\n",
            "copying build/lib.linux-x86_64-3.7/pycocotools/_mask.cpython-37m-x86_64-linux-gnu.so -> pycocotools\n",
            "rm -rf build\n"
          ]
        }
      ],
      "source": [
        "%cd /content/UAVHRBuildingDetection\n",
        "\n",
        "# install cocoapi\n",
        "if not os.path.exists(r\"/content/UAVHRBuildingDetection/Tensorflow/models/research/cocoapi\"):\n",
        "  !git clone https://github.com/cocodataset/cocoapi.git\n",
        "  %cd /content/UAVHRBuildingDetection/cocoapi/PythonAPI\n",
        "  !make\n",
        "  !cp -r pycocotools /content/UAVHRBuildingDetection/Tensorflow/models/research/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LTQ82a_KxUgQ",
        "outputId": "0b559adc-9b71-4b36-b7c0-dc86ce2e6d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection/Tensorflow/models/research\n",
            "\u001b[33mWARNING: --use-feature=2020-resolver no longer has any effect, since it is now the default dependency resolver in pip. This will become an error in pip 21.0.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/UAVHRBuildingDetection/Tensorflow/models/research\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "Collecting avro-python3\n",
            "  Downloading avro-python3-1.10.2.tar.gz (38 kB)\n",
            "Collecting apache-beam\n",
            "  Downloading apache_beam-2.42.0-cp37-cp37m-manylinux2010_x86_64.whl (11.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.0 MB 12.7 MB/s \n",
            "\u001b[?25hCollecting pillow\n",
            "  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 62.8 MB/s \n",
            "\u001b[?25hCollecting lxml\n",
            "  Downloading lxml-4.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 51.5 MB/s \n",
            "\u001b[?25hCollecting matplotlib\n",
            "  Downloading matplotlib-3.5.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (11.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting Cython\n",
            "  Downloading Cython-0.29.32-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 60.9 MB/s \n",
            "\u001b[?25hCollecting contextlib2\n",
            "  Downloading contextlib2-21.6.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting tf-slim\n",
            "  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n",
            "\u001b[K     |████████████████████████████████| 352 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting six\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pycocotools\n",
            "  Downloading pycocotools-2.0.5.tar.gz (24 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lvis\n",
            "  Downloading lvis-0.5.3-py3-none-any.whl (14 kB)\n",
            "Collecting scipy\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.1 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting pandas\n",
            "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 58.6 MB/s \n",
            "\u001b[?25hCollecting tf-models-official>=2.5.1\n",
            "  Downloading tf_models_official-2.10.0-py2.py3-none-any.whl (2.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2 MB 35.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow_io\n",
            "  Downloading tensorflow_io-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (25.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 25.0 MB 93.4 MB/s \n",
            "\u001b[?25hCollecting keras\n",
            "  Downloading keras-2.10.0-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 52.3 MB/s \n",
            "\u001b[?25hCollecting pyparsing==2.4.7\n",
            "  Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.2 MB/s \n",
            "\u001b[?25hCollecting sacrebleu<=2.2.0\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting numpy>=1.17\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 56.5 MB/s \n",
            "\u001b[?25hCollecting tabulate>=0.8.9\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Collecting regex\n",
            "  Downloading regex-2022.9.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[K     |████████████████████████████████| 757 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting portalocker\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting google-api-python-client>=1.6.7\n",
            "  Downloading google_api_python_client-2.65.0-py2.py3-none-any.whl (10.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.4 MB 56.3 MB/s \n",
            "\u001b[?25hCollecting py-cpuinfo>=3.3.0\n",
            "  Downloading py-cpuinfo-8.0.0.tar.gz (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.2 MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless==4.5.2.52\n",
            "  Downloading opencv_python_headless-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 56.2 MB/s \n",
            "\u001b[?25hCollecting pyyaml<6.0,>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 62.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-text~=2.10.0\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 75.3 MB/s \n",
            "\u001b[?25hCollecting tensorflow~=2.10.0\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 15 kB/s \n",
            "\u001b[?25hCollecting oauth2client\n",
            "  Downloading oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 8.5 MB/s \n",
            "\u001b[?25hCollecting kaggle>=1.3.9\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 6.8 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 55.8 MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting immutabledict\n",
            "  Downloading immutabledict-2.2.1-py3-none-any.whl (4.0 kB)\n",
            "Collecting gin-config\n",
            "  Downloading gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 8.6 MB/s \n",
            "\u001b[?25hCollecting tensorflow-model-optimization>=0.4.1\n",
            "  Downloading tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
            "\u001b[K     |████████████████████████████████| 238 kB 73.4 MB/s \n",
            "\u001b[?25hCollecting tensorflow-datasets\n",
            "  Downloading tensorflow_datasets-4.7.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 59.9 MB/s \n",
            "\u001b[?25hCollecting psutil>=5.4.3\n",
            "  Downloading psutil-5.9.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (291 kB)\n",
            "\u001b[K     |████████████████████████████████| 291 kB 68.5 MB/s \n",
            "\u001b[?25hCollecting tensorflow-hub>=0.6.0\n",
            "  Downloading tensorflow_hub-0.12.0-py2.py3-none-any.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 72.5 MB/s \n",
            "\u001b[?25hCollecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5\n",
            "  Downloading google_api_core-2.10.2-py3-none-any.whl (115 kB)\n",
            "\u001b[K     |████████████████████████████████| 115 kB 70.9 MB/s \n",
            "\u001b[?25hCollecting httplib2<1dev,>=0.15.0\n",
            "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting google-auth-httplib2>=0.1.0\n",
            "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
            "Collecting uritemplate<5,>=3.0.1\n",
            "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
            "Collecting google-auth<3.0.0dev,>=1.19.0\n",
            "  Downloading google_auth-2.13.0-py2.py3-none-any.whl (174 kB)\n",
            "\u001b[K     |████████████████████████████████| 174 kB 70.5 MB/s \n",
            "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.56.2\n",
            "  Downloading googleapis_common_protos-1.56.4-py2.py3-none-any.whl (211 kB)\n",
            "\u001b[K     |████████████████████████████████| 211 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0dev,>=2.18.0\n",
            "  Downloading requests-2.28.1-py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.6 MB/s \n",
            "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
            "  Downloading protobuf-4.21.8-cp37-abi3-manylinux2014_x86_64.whl (408 kB)\n",
            "\u001b[K     |████████████████████████████████| 408 kB 65.7 MB/s \n",
            "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
            "\u001b[K     |████████████████████████████████| 155 kB 70.8 MB/s \n",
            "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
            "  Downloading cachetools-5.2.0-py3-none-any.whl (9.3 kB)\n",
            "Collecting certifi\n",
            "  Downloading certifi-2022.9.24-py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 76.2 MB/s \n",
            "\u001b[?25hCollecting python-dateutil\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 74.8 MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 8.1 MB/s \n",
            "\u001b[?25hCollecting python-slugify\n",
            "  Downloading python_slugify-6.1.2-py2.py3-none-any.whl (9.4 kB)\n",
            "Collecting urllib3\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 67.3 MB/s \n",
            "\u001b[?25hCollecting pytz>=2017.3\n",
            "  Downloading pytz-2022.5-py2.py3-none-any.whl (500 kB)\n",
            "\u001b[K     |████████████████████████████████| 500 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting pyasn1<0.5.0,>=0.4.6\n",
            "  Downloading pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.4 MB/s \n",
            "\u001b[?25hCollecting charset-normalizer<3,>=2\n",
            "  Downloading charset_normalizer-2.1.1-py3-none-any.whl (39 kB)\n",
            "Collecting idna<4,>=2.5\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 131 kB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.11,>=2.10.0\n",
            "  Downloading tensorflow_estimator-2.10.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[K     |████████████████████████████████| 438 kB 71.9 MB/s \n",
            "\u001b[?25hCollecting h5py>=2.9.0\n",
            "  Downloading h5py-3.7.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1 MB 55.0 MB/s \n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
            "  Downloading grpcio-1.50.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 63.6 MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.1\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[K     |████████████████████████████████| 57 kB 4.9 MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "  Using cached setuptools-65.5.0-py3-none-any.whl (1.2 MB)\n",
            "Collecting absl-py>=1.0.0\n",
            "  Downloading absl_py-1.3.0-py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 77.8 MB/s \n",
            "\u001b[?25hCollecting gast<=0.4.0,>=0.2.1\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading termcolor-2.0.1-py3-none-any.whl (5.4 kB)\n",
            "Collecting packaging\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting tensorboard<2.11,>=2.10\n",
            "  Downloading tensorboard-2.10.1-py3-none-any.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 62.3 MB/s \n",
            "\u001b[?25hCollecting flatbuffers>=2.0\n",
            "  Downloading flatbuffers-22.9.24-py2.py3-none-any.whl (26 kB)\n",
            "Collecting keras-preprocessing>=1.1.1\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
            "  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 64.9 MB/s \n",
            "\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.23.1\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.27.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[K     |████████████████████████████████| 65 kB 4.5 MB/s \n",
            "\u001b[?25hCollecting astunparse>=1.6.0\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting typing-extensions>=3.6.6\n",
            "  Downloading typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
            "Collecting libclang>=13.0.0\n",
            "  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.1 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.0\n",
            "  Downloading wrapt-1.14.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[K     |████████████████████████████████| 75 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting wheel<1.0,>=0.23.0\n",
            "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
            "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 60.7 MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Collecting werkzeug>=1.0.1\n",
            "  Downloading Werkzeug-2.2.2-py3-none-any.whl (232 kB)\n",
            "\u001b[K     |████████████████████████████████| 232 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[K     |████████████████████████████████| 781 kB 75.7 MB/s \n",
            "\u001b[?25hCollecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
            "\u001b[K     |████████████████████████████████| 93 kB 1.1 MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Collecting importlib-metadata>=4.4\n",
            "  Downloading importlib_metadata-5.0.0-py3-none-any.whl (21 kB)\n",
            "Collecting zipp>=0.5\n",
            "  Downloading zipp-3.9.0-py3-none-any.whl (5.8 kB)\n",
            "Collecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 71.5 MB/s \n",
            "\u001b[?25hCollecting dm-tree~=0.1.1\n",
            "  Downloading dm_tree-0.1.7-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (143 kB)\n",
            "\u001b[K     |████████████████████████████████| 143 kB 73.0 MB/s \n",
            "\u001b[?25hCollecting MarkupSafe>=2.1.1\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Collecting cloudpickle~=2.1.0\n",
            "  Downloading cloudpickle-2.1.0-py3-none-any.whl (25 kB)\n",
            "Collecting pyarrow<8.0.0,>=0.15.1\n",
            "  Downloading pyarrow-7.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 26.7 MB 86.5 MB/s \n",
            "\u001b[?25hCollecting zstandard<1,>=0.18.0\n",
            "  Downloading zstandard-0.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 52.3 MB/s \n",
            "\u001b[?25hCollecting dill<0.3.2,>=0.3.1.1\n",
            "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 76.5 MB/s \n",
            "\u001b[?25hCollecting pymongo<4.0.0,>=3.8.0\n",
            "  Downloading pymongo-3.12.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (508 kB)\n",
            "\u001b[K     |████████████████████████████████| 508 kB 71.8 MB/s \n",
            "\u001b[?25hCollecting hdfs<3.0.0,>=2.1.0\n",
            "  Downloading hdfs-2.7.0-py3-none-any.whl (34 kB)\n",
            "Collecting orjson<4.0\n",
            "  Downloading orjson-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (270 kB)\n",
            "\u001b[K     |████████████████████████████████| 270 kB 75.2 MB/s \n",
            "\u001b[?25hCollecting fastavro<2,>=0.23.6\n",
            "  Downloading fastavro-1.6.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 50.9 MB/s \n",
            "\u001b[?25hCollecting crcmod<2.0,>=1.7\n",
            "  Downloading crcmod-1.7.tar.gz (89 kB)\n",
            "\u001b[K     |████████████████████████████████| 89 kB 9.4 MB/s \n",
            "\u001b[?25hCollecting pydot<2,>=1.2.0\n",
            "  Downloading pydot-1.4.2-py2.py3-none-any.whl (21 kB)\n",
            "Collecting proto-plus<2,>=1.7.1\n",
            "  Downloading proto_plus-1.22.1-py3-none-any.whl (47 kB)\n",
            "\u001b[K     |████████████████████████████████| 47 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting docopt\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "Collecting cycler>=0.10.0\n",
            "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
            "Collecting opencv-python>=4.1.0.25\n",
            "  Downloading opencv_python-4.6.0.66-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.9 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting kiwisolver>=1.1.0\n",
            "  Downloading kiwisolver-1.4.4-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 61.1 MB/s \n",
            "\u001b[?25hCollecting fonttools>=4.22.0\n",
            "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 65.4 MB/s \n",
            "\u001b[?25hCollecting text-unidecode>=1.3\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.21.3\n",
            "  Downloading scikit_learn-1.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (24.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.8 MB 1.3 MB/s \n",
            "\u001b[?25hCollecting joblib>=0.11\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 71.2 MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting typeguard>=2.7\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Collecting importlib-resources\n",
            "  Downloading importlib_resources-5.10.0-py3-none-any.whl (34 kB)\n",
            "Collecting promise\n",
            "  Downloading promise-2.3.tar.gz (19 kB)\n",
            "Collecting tensorflow-metadata\n",
            "  Downloading tensorflow_metadata-1.10.0-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting etils[epath]\n",
            "  Downloading etils-0.8.0-py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 72.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: object-detection, kaggle, py-cpuinfo, crcmod, dill, avro-python3, docopt, pycocotools, seqeval, promise\n",
            "  Building wheel for object-detection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for object-detection: filename=object_detection-0.1-py3-none-any.whl size=1696560 sha256=4123ddce35ba46c857450a7c081bbeeaeefa4cf20ae7321f7a57bf6b1076f360\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-580u8kao/wheels/d1/88/4c/6e266386b82f3ada7b4a0729eeff3721d08a9ba6a71efc7a13\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=30ab9f240079f7517ce256c3a1bd73d00c18f89df4b1412079f342da680ee024\n",
            "  Stored in directory: /root/.cache/pip/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-cpuinfo: filename=py_cpuinfo-8.0.0-py3-none-any.whl size=22257 sha256=013ea370d718a573d5cb9e3ca5a7b18c514b7f845dce4850a46c9dbeee98752e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d2/f1/1f/041add21dc9c4220157f1bd2bd6afe1f1a49524c3396b94401\n",
            "  Building wheel for crcmod (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for crcmod: filename=crcmod-1.7-cp37-cp37m-linux_x86_64.whl size=36350 sha256=2a0de0d99b8b5acf4dc2afddf0f6ab1e7788803b1567c1411f4b35f7695f3fee\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/9a/e9/49e627353476cec8484343c4ab656f1e0d783ee77b9dde2d1f\n",
            "  Building wheel for dill (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78544 sha256=735df9e066deb12144ba68b7844a70879e410c2c8c8d2e1a5da93dd14dca7d32\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
            "  Building wheel for avro-python3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for avro-python3: filename=avro_python3-1.10.2-py3-none-any.whl size=44010 sha256=2e5f4cdb8bb7cd586f4bb5d570ad1317307e04c4ede6132c9fa3a4562065d4b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/e5/b1/6b151d9b535ee50aaa6ab27d145a0104b6df02e5636f0376da\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13723 sha256=7cc942e794e0f92d708cc2625ea420293bc740f7558c11f3f7807aba44b55866\n",
            "  Stored in directory: /root/.cache/pip/wheels/72/b0/3f/1d95f96ff986c7dfffe46ce2be4062f38ebd04b506c77c81b9\n",
            "  Building wheel for pycocotools (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0.5-cp37-cp37m-linux_x86_64.whl size=266474 sha256=4b5b7527c9971c66b87f346afeb6fe5c1c4d69f309f7ab47a4100b92d8e9438b\n",
            "  Stored in directory: /root/.cache/pip/wheels/85/c4/f0/7128093a134f590e4383fd60cb484960878721d98b9a515317\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16180 sha256=ade8fa3db557855ff445469ce0f819365b26b6e10d9025f7c4c3dab6b9baf974\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n",
            "  Building wheel for promise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for promise: filename=promise-2.3-py3-none-any.whl size=21503 sha256=b557d0558cc98260f2940c885ef65274e8cc3916673e8c6736cc2a87a5ec4ae2\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/93/c6/762e359f8cb6a5b69c72235d798804cae523bbe41c2aa8333d\n",
            "Successfully built object-detection kaggle py-cpuinfo crcmod dill avro-python3 docopt pycocotools seqeval promise\n",
            "Installing collected packages: urllib3, pyasn1, idna, charset-normalizer, certifi, zipp, typing-extensions, six, rsa, requests, pyasn1-modules, oauthlib, cachetools, requests-oauthlib, MarkupSafe, importlib-metadata, google-auth, wheel, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, setuptools, pyparsing, protobuf, numpy, markdown, grpcio, google-auth-oauthlib, etils, absl-py, wrapt, threadpoolctl, text-unidecode, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, scipy, python-dateutil, pillow, packaging, opt-einsum, libclang, kiwisolver, keras-preprocessing, keras, joblib, importlib-resources, httplib2, h5py, googleapis-common-protos, google-pasta, gast, fonttools, flatbuffers, cycler, astunparse, uritemplate, typeguard, tqdm, toml, tensorflow-metadata, tensorflow-hub, tensorflow, tabulate, scikit-learn, regex, pytz, python-slugify, promise, portalocker, matplotlib, lxml, google-auth-httplib2, google-api-core, docopt, dm-tree, dill, colorama, zstandard, tf-slim, tensorflow-text, tensorflow-model-optimization, tensorflow-datasets, tensorflow-addons, seqeval, sentencepiece, sacrebleu, pyyaml, pymongo, pydot, pycocotools, pyarrow, py-cpuinfo, psutil, proto-plus, pandas, orjson, opencv-python-headless, opencv-python, oauth2client, kaggle, immutabledict, hdfs, google-api-python-client, gin-config, fastavro, Cython, crcmod, cloudpickle, tf-models-official, tensorflow-io, lvis, contextlib2, avro-python3, apache-beam, object-detection\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.9.0 requires jedi>=0.10, which is not installed.\n",
            "thinc 8.1.4 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "spacy 3.4.1 requires typing-extensions<4.2.0,>=3.7.4; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\n",
            "google-cloud-translate 1.5.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-language 1.2.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-firestore 1.7.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-datastore 1.8.0 requires google-api-core[grpc]<2.0.0dev,>=1.6.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "google-cloud-core 1.0.3 requires google-api-core<2.0.0dev,>=1.14.0, but you have google-api-core 2.10.2 which is incompatible.\n",
            "flask 1.1.4 requires Werkzeug<2.0,>=0.15, but you have werkzeug 2.2.2 which is incompatible.\n",
            "firebase-admin 4.4.0 requires google-api-core[grpc]<2.0.0dev,>=1.14.0; platform_python_implementation != \"PyPy\", but you have google-api-core 2.10.2 which is incompatible.\n",
            "earthengine-api 0.1.327 requires google-api-python-client<2,>=1.12.1, but you have google-api-python-client 2.65.0 which is incompatible.\n",
            "confection 0.0.3 requires typing-extensions<4.2.0,>=3.7.4.1; python_version < \"3.8\", but you have typing-extensions 4.4.0 which is incompatible.\u001b[0m\n",
            "Successfully installed Cython-0.29.32 MarkupSafe-2.1.1 absl-py-1.3.0 apache-beam-2.42.0 astunparse-1.6.3 avro-python3-1.10.2 cachetools-5.2.0 certifi-2022.9.24 charset-normalizer-2.1.1 cloudpickle-2.1.0 colorama-0.4.5 contextlib2-21.6.0 crcmod-1.7 cycler-0.11.0 dill-0.3.5.1 dm-tree-0.1.7 docopt-0.6.2 etils-0.8.0 fastavro-1.6.1 flatbuffers-22.9.24 fonttools-4.38.0 gast-0.4.0 gin-config-0.5.0 google-api-core-2.10.2 google-api-python-client-2.65.0 google-auth-2.13.0 google-auth-httplib2-0.1.0 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 googleapis-common-protos-1.56.4 grpcio-1.50.0 h5py-3.7.0 hdfs-2.7.0 httplib2-0.20.4 idna-3.4 immutabledict-2.2.1 importlib-metadata-5.0.0 importlib-resources-5.10.0 joblib-1.2.0 kaggle-1.5.12 keras-2.10.0 keras-preprocessing-1.1.2 kiwisolver-1.4.4 libclang-14.0.6 lvis-0.5.3 lxml-4.9.1 markdown-3.4.1 matplotlib-3.5.3 numpy-1.21.6 oauth2client-4.1.3 oauthlib-3.2.2 object-detection-0.1 opencv-python-4.6.0.66 opencv-python-headless-4.6.0.66 opt-einsum-3.3.0 orjson-3.8.0 packaging-21.3 pandas-1.3.5 pillow-9.2.0 portalocker-2.6.0 promise-2.3 proto-plus-1.22.1 protobuf-3.19.6 psutil-5.9.3 py-cpuinfo-8.0.0 pyarrow-7.0.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 pycocotools-2.0.5 pydot-1.4.2 pymongo-4.2.0 pyparsing-3.0.9 python-dateutil-2.8.2 python-slugify-6.1.2 pytz-2022.5 pyyaml-6.0 regex-2022.9.13 requests-2.28.1 requests-oauthlib-1.3.1 rsa-4.9 sacrebleu-2.2.0 scikit-learn-1.0.2 scipy-1.7.3 sentencepiece-0.1.97 seqeval-1.2.2 setuptools-65.5.0 six-1.16.0 tabulate-0.9.0 tensorboard-2.10.1 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.10.0 tensorflow-addons-0.18.0 tensorflow-datasets-4.7.0 tensorflow-estimator-2.10.0 tensorflow-hub-0.12.0 tensorflow-io-0.27.0 tensorflow-io-gcs-filesystem-0.27.0 tensorflow-metadata-1.10.0 tensorflow-model-optimization-0.7.3 tensorflow-text-2.10.0 termcolor-2.0.1 text-unidecode-1.3 tf-models-official-2.10.0 tf-slim-1.1.0 threadpoolctl-3.1.0 toml-0.10.2 tqdm-4.64.1 typeguard-2.13.3 typing-extensions-4.4.0 uritemplate-4.1.1 urllib3-1.26.12 werkzeug-2.2.2 wheel-0.37.1 wrapt-1.14.1 zipp-3.9.0 zstandard-0.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "cycler",
                  "dateutil",
                  "google",
                  "httplib2",
                  "kiwisolver",
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy",
                  "pkg_resources",
                  "psutil",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install object detection api\n",
        "%cd /content/UAVHRBuildingDetection/Tensorflow/models/research/\n",
        "!cp object_detection/packages/tf2/setup.py .\n",
        "!pip install --ignore-installed --use-feature=2020-resolver ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVHMexLgxV_6",
        "outputId": "fa6f4c92-7dc3-49af-fb88-bbffcac75598"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection/Tensorflow/models/research\n",
            "2022-10-22 20:55:23.426940: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-10-22 20:55:23.604700: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-22 20:55:24.482656: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-22 20:55:24.482845: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-22 20:55:24.482885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-10-22 20:55:27.950786: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:28.120603: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:28.121311: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "Running tests under Python 3.7.15: /usr/bin/python3\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "2022-10-22 20:55:28.128759: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-10-22 20:55:28.129067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:28.129692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:28.130314: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:29.107377: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:29.108167: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:29.108782: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2022-10-22 20:55:29.109312: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2022-10-22 20:55:29.109363: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13735 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "W1022 20:55:29.339182 140502691698560 model_builder.py:1109] Building experimental DeepMAC meta-arch. Some features may be omitted.\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.48s\n",
            "I1022 20:55:29.606997 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_center_net_deepmac): 1.48s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_deepmac\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.67s\n",
            "I1022 20:55:30.281418 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)): 0.67s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model0 (customize_head_params=True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
            "I1022 20:55:30.557528 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)): 0.28s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model1 (customize_head_params=False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.22s\n",
            "I1022 20:55:30.783057 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_from_keypoints): 0.22s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_from_keypoints\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.5s\n",
            "I1022 20:55:33.282270 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_center_net_model_mobilenet): 2.5s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_center_net_model_mobilenet\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "I1022 20:55:33.295282 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_experimental_model): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_experimental_model\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.05s\n",
            "I1022 20:55:33.348688 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)): 0.05s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature0 (True)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
            "I1022 20:55:33.375548 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)): 0.03s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_from_config_with_crop_feature1 (False)\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "I1022 20:55:33.400847 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner): 0.02s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_model_from_config_with_example_miner\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.22s\n",
            "I1022 20:55:33.617558 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul): 0.22s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.15s\n",
            "I1022 20:55:33.763807 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul): 0.15s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_faster_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "I1022 20:55:33.921574 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_with_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.17s\n",
            "I1022 20:55:34.091062 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul): 0.17s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_faster_rcnn_models_from_config_mask_rcnn_without_matmul\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "I1022 20:55:34.249614 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_rfcn_model_from_config): 0.16s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_rfcn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.31s\n",
            "I1022 20:55:34.564726 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config): 0.31s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_fpn_model_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "I1022 20:55:34.904066 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b0\n",
            "I1022 20:55:34.905377 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 64\n",
            "I1022 20:55:34.905512 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 3\n",
            "I1022 20:55:34.909203 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:35.022192 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:35.022388 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:35.167544 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:35.167723 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:35.523739 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:35.523951 140502691698560 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1022 20:55:35.852929 140502691698560 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1022 20:55:35.853184 140502691698560 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1022 20:55:36.278100 140502691698560 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1022 20:55:36.278301 140502691698560 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1022 20:55:36.772352 140502691698560 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1022 20:55:36.772592 140502691698560 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1022 20:55:37.533983 140502691698560 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1022 20:55:37.534256 140502691698560 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I1022 20:55:37.706341 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I1022 20:55:37.769753 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.0, resolution=224, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:37.883337 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b1\n",
            "I1022 20:55:37.883564 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 88\n",
            "I1022 20:55:37.883665 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 4\n",
            "I1022 20:55:37.886241 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:37.911469 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:37.911611 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:38.242769 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:38.242951 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:38.502520 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:38.502679 140502691698560 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1022 20:55:38.769735 140502691698560 efficientnet_model.py:143] round_filter input=40 output=40\n",
            "I1022 20:55:38.769908 140502691698560 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1022 20:55:39.119685 140502691698560 efficientnet_model.py:143] round_filter input=80 output=80\n",
            "I1022 20:55:39.119852 140502691698560 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1022 20:55:39.473926 140502691698560 efficientnet_model.py:143] round_filter input=112 output=112\n",
            "I1022 20:55:39.474166 140502691698560 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1022 20:55:39.923794 140502691698560 efficientnet_model.py:143] round_filter input=192 output=192\n",
            "I1022 20:55:39.923970 140502691698560 efficientnet_model.py:143] round_filter input=320 output=320\n",
            "I1022 20:55:40.111996 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=1280\n",
            "I1022 20:55:40.146167 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.0, depth_coefficient=1.1, resolution=240, dropout_rate=0.2, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:40.206804 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b2\n",
            "I1022 20:55:40.206987 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 112\n",
            "I1022 20:55:40.207064 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 5\n",
            "I1022 20:55:40.208862 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:40.226938 140502691698560 efficientnet_model.py:143] round_filter input=32 output=32\n",
            "I1022 20:55:40.227058 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:40.366125 140502691698560 efficientnet_model.py:143] round_filter input=16 output=16\n",
            "I1022 20:55:40.366297 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:40.630978 140502691698560 efficientnet_model.py:143] round_filter input=24 output=24\n",
            "I1022 20:55:40.631162 140502691698560 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1022 20:55:40.919683 140502691698560 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1022 20:55:40.919845 140502691698560 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I1022 20:55:41.291887 140502691698560 efficientnet_model.py:143] round_filter input=80 output=88\n",
            "I1022 20:55:41.292062 140502691698560 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I1022 20:55:41.659213 140502691698560 efficientnet_model.py:143] round_filter input=112 output=120\n",
            "I1022 20:55:41.659384 140502691698560 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I1022 20:55:42.311228 140502691698560 efficientnet_model.py:143] round_filter input=192 output=208\n",
            "I1022 20:55:42.311400 140502691698560 efficientnet_model.py:143] round_filter input=320 output=352\n",
            "I1022 20:55:42.500499 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=1408\n",
            "I1022 20:55:42.540600 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.1, depth_coefficient=1.2, resolution=260, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:42.599916 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b3\n",
            "I1022 20:55:42.600064 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 160\n",
            "I1022 20:55:42.600148 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 6\n",
            "I1022 20:55:42.601824 140502691698560 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I1022 20:55:42.625135 140502691698560 efficientnet_model.py:143] round_filter input=32 output=40\n",
            "I1022 20:55:42.625241 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:42.783594 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:42.783752 140502691698560 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1022 20:55:43.060436 140502691698560 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1022 20:55:43.060607 140502691698560 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1022 20:55:43.337615 140502691698560 efficientnet_model.py:143] round_filter input=40 output=48\n",
            "I1022 20:55:43.337792 140502691698560 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I1022 20:55:43.801313 140502691698560 efficientnet_model.py:143] round_filter input=80 output=96\n",
            "I1022 20:55:43.801543 140502691698560 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I1022 20:55:44.269417 140502691698560 efficientnet_model.py:143] round_filter input=112 output=136\n",
            "I1022 20:55:44.269594 140502691698560 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I1022 20:55:44.817497 140502691698560 efficientnet_model.py:143] round_filter input=192 output=232\n",
            "I1022 20:55:44.817709 140502691698560 efficientnet_model.py:143] round_filter input=320 output=384\n",
            "I1022 20:55:45.009320 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=1536\n",
            "I1022 20:55:45.050193 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.2, depth_coefficient=1.4, resolution=300, dropout_rate=0.3, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:45.110525 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b4\n",
            "I1022 20:55:45.110657 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 224\n",
            "I1022 20:55:45.110724 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I1022 20:55:45.112221 140502691698560 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1022 20:55:45.134786 140502691698560 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1022 20:55:45.134897 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:45.277595 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:45.277739 140502691698560 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1022 20:55:45.644148 140502691698560 efficientnet_model.py:143] round_filter input=24 output=32\n",
            "I1022 20:55:45.644303 140502691698560 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I1022 20:55:46.027938 140502691698560 efficientnet_model.py:143] round_filter input=40 output=56\n",
            "I1022 20:55:46.028120 140502691698560 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I1022 20:55:46.572852 140502691698560 efficientnet_model.py:143] round_filter input=80 output=112\n",
            "I1022 20:55:46.573022 140502691698560 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I1022 20:55:47.124830 140502691698560 efficientnet_model.py:143] round_filter input=112 output=160\n",
            "I1022 20:55:47.125008 140502691698560 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I1022 20:55:47.837322 140502691698560 efficientnet_model.py:143] round_filter input=192 output=272\n",
            "I1022 20:55:47.837501 140502691698560 efficientnet_model.py:143] round_filter input=320 output=448\n",
            "I1022 20:55:48.025519 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=1792\n",
            "I1022 20:55:48.062044 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.4, depth_coefficient=1.8, resolution=380, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:48.130418 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b5\n",
            "I1022 20:55:48.130559 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 288\n",
            "I1022 20:55:48.130625 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 7\n",
            "I1022 20:55:48.132162 140502691698560 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1022 20:55:48.152458 140502691698560 efficientnet_model.py:143] round_filter input=32 output=48\n",
            "I1022 20:55:48.152564 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:48.360235 140502691698560 efficientnet_model.py:143] round_filter input=16 output=24\n",
            "I1022 20:55:48.360381 140502691698560 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1022 20:55:48.797401 140502691698560 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1022 20:55:48.797574 140502691698560 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I1022 20:55:49.495903 140502691698560 efficientnet_model.py:143] round_filter input=40 output=64\n",
            "I1022 20:55:49.496088 140502691698560 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I1022 20:55:50.145209 140502691698560 efficientnet_model.py:143] round_filter input=80 output=128\n",
            "I1022 20:55:50.145379 140502691698560 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I1022 20:55:50.775856 140502691698560 efficientnet_model.py:143] round_filter input=112 output=176\n",
            "I1022 20:55:50.776037 140502691698560 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I1022 20:55:51.603477 140502691698560 efficientnet_model.py:143] round_filter input=192 output=304\n",
            "I1022 20:55:51.603652 140502691698560 efficientnet_model.py:143] round_filter input=320 output=512\n",
            "I1022 20:55:51.883973 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=2048\n",
            "I1022 20:55:51.923750 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.6, depth_coefficient=2.2, resolution=456, dropout_rate=0.4, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:52.016392 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b6\n",
            "I1022 20:55:52.016550 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 384\n",
            "I1022 20:55:52.016647 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I1022 20:55:52.018255 140502691698560 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I1022 20:55:52.038096 140502691698560 efficientnet_model.py:143] round_filter input=32 output=56\n",
            "I1022 20:55:52.038221 140502691698560 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1022 20:55:52.264747 140502691698560 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1022 20:55:52.264926 140502691698560 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1022 20:55:52.806172 140502691698560 efficientnet_model.py:143] round_filter input=24 output=40\n",
            "I1022 20:55:52.806349 140502691698560 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I1022 20:55:53.357780 140502691698560 efficientnet_model.py:143] round_filter input=40 output=72\n",
            "I1022 20:55:53.357971 140502691698560 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I1022 20:55:54.092307 140502691698560 efficientnet_model.py:143] round_filter input=80 output=144\n",
            "I1022 20:55:54.092490 140502691698560 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I1022 20:55:54.811379 140502691698560 efficientnet_model.py:143] round_filter input=112 output=200\n",
            "I1022 20:55:54.811556 140502691698560 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I1022 20:55:55.801880 140502691698560 efficientnet_model.py:143] round_filter input=192 output=344\n",
            "I1022 20:55:55.802057 140502691698560 efficientnet_model.py:143] round_filter input=320 output=576\n",
            "I1022 20:55:56.093070 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=2304\n",
            "I1022 20:55:56.129617 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=1.8, depth_coefficient=2.6, resolution=528, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "I1022 20:55:56.228105 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:150] EfficientDet EfficientNet backbone version: efficientnet-b7\n",
            "I1022 20:55:56.228254 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:151] EfficientDet BiFPN num filters: 384\n",
            "I1022 20:55:56.228329 140502691698560 ssd_efficientnet_bifpn_feature_extractor.py:153] EfficientDet BiFPN num iterations: 8\n",
            "I1022 20:55:56.229995 140502691698560 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I1022 20:55:56.249942 140502691698560 efficientnet_model.py:143] round_filter input=32 output=64\n",
            "I1022 20:55:56.250058 140502691698560 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1022 20:55:56.796630 140502691698560 efficientnet_model.py:143] round_filter input=16 output=32\n",
            "I1022 20:55:56.796810 140502691698560 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I1022 20:55:57.436355 140502691698560 efficientnet_model.py:143] round_filter input=24 output=48\n",
            "I1022 20:55:57.436534 140502691698560 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I1022 20:55:58.090879 140502691698560 efficientnet_model.py:143] round_filter input=40 output=80\n",
            "I1022 20:55:58.091061 140502691698560 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I1022 20:55:58.997217 140502691698560 efficientnet_model.py:143] round_filter input=80 output=160\n",
            "I1022 20:55:58.997388 140502691698560 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I1022 20:55:59.920526 140502691698560 efficientnet_model.py:143] round_filter input=112 output=224\n",
            "I1022 20:55:59.920701 140502691698560 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I1022 20:56:01.118864 140502691698560 efficientnet_model.py:143] round_filter input=192 output=384\n",
            "I1022 20:56:01.119046 140502691698560 efficientnet_model.py:143] round_filter input=320 output=640\n",
            "I1022 20:56:01.488981 140502691698560 efficientnet_model.py:143] round_filter input=1280 output=2560\n",
            "I1022 20:56:01.526169 140502691698560 efficientnet_model.py:453] Building model efficientnet with params ModelConfig(width_coefficient=2.0, depth_coefficient=3.1, resolution=600, dropout_rate=0.5, blocks=(BlockConfig(input_filters=32, output_filters=16, kernel_size=3, num_repeat=1, expand_ratio=1, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=16, output_filters=24, kernel_size=3, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=24, output_filters=40, kernel_size=5, num_repeat=2, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=40, output_filters=80, kernel_size=3, num_repeat=3, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=80, output_filters=112, kernel_size=5, num_repeat=3, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=112, output_filters=192, kernel_size=5, num_repeat=4, expand_ratio=6, strides=(2, 2), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise'), BlockConfig(input_filters=192, output_filters=320, kernel_size=3, num_repeat=1, expand_ratio=6, strides=(1, 1), se_ratio=0.25, id_skip=True, fused_conv=False, conv_type='depthwise')), stem_base_filters=32, top_base_filters=1280, activation='simple_swish', batch_norm='default', bn_momentum=0.99, bn_epsilon=0.001, weight_decay=5e-06, drop_connect_rate=0.2, depth_divisor=8, min_depth=None, use_se=True, input_channels=3, num_classes=1000, model_name='efficientnet', rescale_input=False, data_format='channels_last', dtype='float32')\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 27.06s\n",
            "I1022 20:56:01.633777 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_create_ssd_models_from_config): 27.06s\n",
            "[       OK ] ModelBuilderTF2Test.test_create_ssd_models_from_config\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "I1022 20:56:01.659488 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_faster_rcnn_batchnorm_update\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "I1022 20:56:01.661097 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_first_stage_nms_iou_threshold\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "I1022 20:56:01.661585 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_invalid_model_config_proto): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_model_config_proto\n",
            "[ RUN      ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "I1022 20:56:01.662976 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_invalid_second_stage_batch_size): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_invalid_second_stage_batch_size\n",
            "[ RUN      ] ModelBuilderTF2Test.test_session\n",
            "[  SKIPPED ] ModelBuilderTF2Test.test_session\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "I1022 20:56:01.664301 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_faster_rcnn_feature_extractor\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "I1022 20:56:01.664751 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_unknown_meta_architecture): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_meta_architecture\n",
            "[ RUN      ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "INFO:tensorflow:time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "I1022 20:56:01.665754 140502691698560 test_util.py:2461] time(__main__.ModelBuilderTF2Test.test_unknown_ssd_feature_extractor): 0.0s\n",
            "[       OK ] ModelBuilderTF2Test.test_unknown_ssd_feature_extractor\n",
            "----------------------------------------------------------------------\n",
            "Ran 24 tests in 33.540s\n",
            "\n",
            "OK (skipped=1)\n",
            "/content/UAVHRBuildingDetection\n"
          ]
        }
      ],
      "source": [
        "# Verify installation\n",
        "%cd /content/UAVHRBuildingDetection/Tensorflow/models/research/\n",
        "!python object_detection/builders/model_builder_tf2_test.py\n",
        "\n",
        "# return to content directory\n",
        "%cd /content/UAVHRBuildingDetection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDNCmFR1CnSt"
      },
      "source": [
        "Create directories for pretrained models and custom models in workspace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WMLy0Wx3Csxj"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(paths[\"pretrained_models\"]):\n",
        "  os.makedirs(paths[\"pretrained_models\"])\n",
        "\n",
        "if not os.path.exists(paths[\"custom_models\"]):\n",
        "  os.makedirs(paths[\"custom_models\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P267uD75lE2_"
      },
      "source": [
        "Install other python packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvHMyrW6lI4r",
        "outputId": "d0853140-9c0b-4b05-bfc0-fa3972c517d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (3.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting opencv-python==4.5.2.52\n",
            "  Downloading opencv_python-4.5.2.52-cp37-cp37m-manylinux2014_x86_64.whl (51.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 51.0 MB 197 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python==4.5.2.52) (1.21.6)\n",
            "Installing collected packages: opencv-python\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.6.0.66\n",
            "    Uninstalling opencv-python-4.6.0.66:\n",
            "      Successfully uninstalled opencv-python-4.6.0.66\n",
            "Successfully installed opencv-python-4.6.0.66\n"
          ]
        }
      ],
      "source": [
        "!pip install wget\n",
        "!pip install opencv-python==4.5.2.52\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIUttk2BkFi-"
      },
      "source": [
        "# Download pre-trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sLzYy5BkLg_",
        "outputId": "a91ba17a-11c4-46ca-f975-7dedd7792f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection/Tensorflow/workspace/pretrained_model\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/checkpoint\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/checkpoint/ckpt-0.index\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/pipeline.config\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/saved_model.pb\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.data-00000-of-00001\n",
            "ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8/saved_model/variables/variables.index\n",
            "/content/UAVHRBuildingDetection\n"
          ]
        }
      ],
      "source": [
        "import wget\n",
        "# download pretrained model\n",
        "%cd {paths[\"pretrained_models\"]}\n",
        "preModZipName = PRETRAINED_MODEL_NAME + \".tar.gz\"\n",
        "\n",
        "if not os.path.exists(preModZipName):\n",
        "  wget.download(PRETRAINED_MODEL_URL)\n",
        "\n",
        "if not os.path.exists(PRETRAINED_MODEL_NAME):\n",
        "  !tar -zxvf {PRETRAINED_MODEL_NAME + \".tar.gz\"}\n",
        "\n",
        "%cd {paths[\"github_repo\"]}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleanup previous training files"
      ],
      "metadata": {
        "id": "5PjjoyyzONMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from shutil import rmtree\n",
        "if os.path.exists(paths[\"custom_model_dir\"]):\n",
        "  rmtree(paths[\"custom_model_dir\"])\n",
        "\n",
        "if os.path.exists(paths[\"images\"]):\n",
        "  rmtree(paths[\"images\"])"
      ],
      "metadata": {
        "id": "w8yTQtotORhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAlZZ0yXyoQ6"
      },
      "source": [
        "# Setup Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extract Dataset"
      ],
      "metadata": {
        "id": "EnW1Gz4vXgM2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tHwGkWc66aC",
        "outputId": "6a4969a8-7136-4383-fab9-fcc52ca302ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection\n",
            "/content/UAVHRBuildingDetection/Tensorflow/workspace\n",
            "annotations/\n",
            "annotations/labelmap.pbtxt\n",
            "annotations/test.record\n",
            "annotations/train.record\n",
            "annotations/valid.record\n",
            "/content/UAVHRBuildingDetection\n"
          ]
        }
      ],
      "source": [
        "from shutil import copyfile\n",
        "from shutil import rmtree\n",
        "\n",
        "%cd {paths[\"github_repo\"]}\n",
        "\n",
        "# Copy compressed data\n",
        "if os.path.exists(paths[\"annotations\"]):\n",
        "  rmtree(paths[\"annotations\"])\n",
        "\n",
        "copyfile(os.path.join(paths[\"gdrive_data\"], \"annotations.tar.gz\"), os.path.join(paths[\"workspace\"],\"annotations.tar.gz\"))\n",
        "\n",
        "# Decompress Data\n",
        "%cd {paths[\"workspace\"]}\n",
        "!tar -zxvf {\"annotations.tar.gz\"}\n",
        "\n",
        "# Return to project repo directory\n",
        "%cd {paths[\"github_repo\"]}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEWJr8-M9qBB"
      },
      "source": [
        "copy and adapt pipline config if necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "zMHo2Mfp9umt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from object_detection.utils import config_util\n",
        "from object_detection.protos import pipeline_pb2\n",
        "from google.protobuf import text_format\n",
        "from shutil import rmtree\n",
        "\n",
        "if not os.path.exists(paths[\"custom_model_dir\"] ):\n",
        "  os.makedirs(paths[\"custom_model_dir\"])\n",
        "\n",
        "if not os.path.exists(paths[\"custom_model_config\"]):\n",
        "  # Copy Config if necessary\n",
        "  if os.name == \"posix\":\n",
        "    !cp {os.path.join(paths[\"pretrained_models\"], PRETRAINED_MODEL_NAME,\"pipeline.config\")} {paths[\"custom_model_config\"]}\n",
        "  elif os.name == \"nt\":\n",
        "    !copy {os.path.join(paths[\"pretrained_models\"], PRETRAINED_MODEL_NAME,\"pipeline.config\")} {paths[\"custom_model_config\"]}\n",
        "  \n",
        "\n",
        "  # adapt config\n",
        "  config = config_util.get_configs_from_pipeline_file(paths[\"custom_model_config\"])\n",
        "\n",
        "  pipeline_config = pipeline_pb2.TrainEvalPipelineConfig()\n",
        "  with tf.io.gfile.GFile(paths['custom_model_config'], \"r\") as f:                                                                                                                                                                                                                     \n",
        "      proto_str = f.read()                                                                                                                                                                                                                                          \n",
        "      text_format.Merge(proto_str, pipeline_config)  \n",
        "\n",
        "  pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.height = INPUT_DIMS[0]\n",
        "  pipeline_config.model.ssd.image_resizer.fixed_shape_resizer.width = INPUT_DIMS[1]\n",
        "  pipeline_config.model.ssd.num_classes = NOF_CLASSES\n",
        "  pipeline_config.train_config.batch_size = BATCH_SIZE\n",
        "  pipeline_config.train_config.fine_tune_checkpoint = os.path.join(paths[\"pretrained_models\"], PRETRAINED_MODEL_NAME, 'checkpoint', 'ckpt-0')\n",
        "  pipeline_config.train_config.fine_tune_checkpoint_type = \"detection\"\n",
        "  pipeline_config.train_input_reader.label_map_path= os.path.join(paths[\"annotations\"],LABEL_MAP_NAME)\n",
        "  pipeline_config.train_input_reader.tf_record_input_reader.input_path[:] = [os.path.join(paths[\"annotations\"], 'train.record')]\n",
        "  pipeline_config.eval_input_reader[0].label_map_path = os.path.join(paths[\"annotations\"],LABEL_MAP_NAME)\n",
        "  pipeline_config.eval_input_reader[0].tf_record_input_reader.input_path[:] = [os.path.join(paths[\"annotations\"], 'test.record')]\n",
        "\n",
        "  pipeline_config.eval_config.num_visualizations=100\n",
        "  pipeline_config.eval_config.max_num_boxes_to_visualize = 100\n",
        "  pipeline_config.eval_config.num_examples = SIZE_TEST_VALID_SET\n",
        "  pipeline_config.eval_config.eval_interval_secs = 10\n",
        "  pipeline_config.eval_config.batch_size = BATCH_SIZE\n",
        "\n",
        "\n",
        "\n",
        "  config_text = text_format.MessageToString(pipeline_config)                                                                                                                                                                                                        \n",
        "  with tf.io.gfile.GFile(paths[\"custom_model_config\"], \"wb\") as f:                                                                                                                                                                                                                     \n",
        "      f.write(config_text)   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpTdLaJ5O59u"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XCP97JHVO-56",
        "outputId": "11e7aa2f-f0cf-48c1-9148-ce23ef74922d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-10-22 21:06:30.346959: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2022-10-22 21:06:31.617932: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-22 21:06:31.618104: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/lib64-nvidia\n",
            "2022-10-22 21:06:31.618128: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
            "2022-10-22 21:06:37.169595: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "I1022 21:06:37.196661 140337659815808 mirrored_strategy.py:374] Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
            "INFO:tensorflow:Maybe overwriting train_steps: 20000\n",
            "I1022 21:06:37.203467 140337659815808 config_util.py:552] Maybe overwriting train_steps: 20000\n",
            "INFO:tensorflow:Maybe overwriting use_bfloat16: False\n",
            "I1022 21:06:37.203664 140337659815808 config_util.py:552] Maybe overwriting use_bfloat16: False\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "W1022 21:06:37.250815 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/model_lib_v2.py:564: StrategyBase.experimental_distribute_datasets_from_function (from tensorflow.python.distribute.distribute_lib) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "rename to distribute_datasets_from_function\n",
            "INFO:tensorflow:Reading unweighted datasets: ['//content/UAVHRBuildingDetection/Tensorflow/workspace/annotations/train.record']\n",
            "I1022 21:06:37.260760 140337659815808 dataset_builder.py:162] Reading unweighted datasets: ['//content/UAVHRBuildingDetection/Tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Reading record datasets for input file: ['//content/UAVHRBuildingDetection/Tensorflow/workspace/annotations/train.record']\n",
            "I1022 21:06:37.260987 140337659815808 dataset_builder.py:79] Reading record datasets for input file: ['//content/UAVHRBuildingDetection/Tensorflow/workspace/annotations/train.record']\n",
            "INFO:tensorflow:Number of filenames to read: 1\n",
            "I1022 21:06:37.261158 140337659815808 dataset_builder.py:80] Number of filenames to read: 1\n",
            "WARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\n",
            "W1022 21:06:37.261264 140337659815808 dataset_builder.py:87] num_readers has been reduced to 1 to match input file shards.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "W1022 21:06:37.269804 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:104: parallel_interleave (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.interleave(map_func, cycle_length, block_length, num_parallel_calls=tf.data.AUTOTUNE)` instead. If sloppy execution is desired, use `tf.data.Options.deterministic`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "W1022 21:06:37.303009 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/object_detection/builders/dataset_builder.py:236: DatasetV1.map_with_legacy_function (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.data.Dataset.map()\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "W1022 21:06:48.402623 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "W1022 21:06:52.764584 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: sample_distorted_bounding_box (from tensorflow.python.ops.image_ops_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "`seed2` arg is deprecated.Use sample_distorted_bounding_box_v2 instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "W1022 21:06:55.462343 140337659815808 deprecation.py:356] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:452: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  \"`tf.keras.backend.set_learning_phase` is deprecated and \"\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.966343 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.970593 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.974693 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.976198 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.981307 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.983986 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.989164 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.990599 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.994539 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "I1022 21:07:43.995955 140337659815808 cross_device_ops.py:618] Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "W1022 21:07:45.904822 140333182592768 deprecation.py:560] From /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/deprecation.py:629: calling map_fn_v2 (from tensorflow.python.ops.map_fn) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use fn_output_signature instead\n",
            "INFO:tensorflow:Step 100 per-step time 1.303s\n",
            "I1022 21:09:55.480409 140337659815808 model_lib_v2.py:707] Step 100 per-step time 1.303s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.3336386,\n",
            " 'Loss/localization_loss': 0.41412184,\n",
            " 'Loss/regularization_loss': 0.15340827,\n",
            " 'Loss/total_loss': 0.9011687,\n",
            " 'learning_rate': 0.0319994}\n",
            "I1022 21:09:55.480850 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.3336386,\n",
            " 'Loss/localization_loss': 0.41412184,\n",
            " 'Loss/regularization_loss': 0.15340827,\n",
            " 'Loss/total_loss': 0.9011687,\n",
            " 'learning_rate': 0.0319994}\n",
            "INFO:tensorflow:Step 200 per-step time 0.408s\n",
            "I1022 21:10:36.146089 140337659815808 model_lib_v2.py:707] Step 200 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.252931,\n",
            " 'Loss/localization_loss': 0.27698034,\n",
            " 'Loss/regularization_loss': 0.15322086,\n",
            " 'Loss/total_loss': 0.6831322,\n",
            " 'learning_rate': 0.0373328}\n",
            "I1022 21:10:36.146494 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.252931,\n",
            " 'Loss/localization_loss': 0.27698034,\n",
            " 'Loss/regularization_loss': 0.15322086,\n",
            " 'Loss/total_loss': 0.6831322,\n",
            " 'learning_rate': 0.0373328}\n",
            "INFO:tensorflow:Step 300 per-step time 0.409s\n",
            "I1022 21:11:17.044553 140337659815808 model_lib_v2.py:707] Step 300 per-step time 0.409s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.245778,\n",
            " 'Loss/localization_loss': 0.25616533,\n",
            " 'Loss/regularization_loss': 0.1530139,\n",
            " 'Loss/total_loss': 0.65495723,\n",
            " 'learning_rate': 0.0426662}\n",
            "I1022 21:11:17.045239 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.245778,\n",
            " 'Loss/localization_loss': 0.25616533,\n",
            " 'Loss/regularization_loss': 0.1530139,\n",
            " 'Loss/total_loss': 0.65495723,\n",
            " 'learning_rate': 0.0426662}\n",
            "INFO:tensorflow:Step 400 per-step time 0.406s\n",
            "I1022 21:11:57.593425 140337659815808 model_lib_v2.py:707] Step 400 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.2679119,\n",
            " 'Loss/localization_loss': 0.16699904,\n",
            " 'Loss/regularization_loss': 0.15284158,\n",
            " 'Loss/total_loss': 0.5877525,\n",
            " 'learning_rate': 0.047999598}\n",
            "I1022 21:11:57.593854 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.2679119,\n",
            " 'Loss/localization_loss': 0.16699904,\n",
            " 'Loss/regularization_loss': 0.15284158,\n",
            " 'Loss/total_loss': 0.5877525,\n",
            " 'learning_rate': 0.047999598}\n",
            "INFO:tensorflow:Step 500 per-step time 0.407s\n",
            "I1022 21:12:38.317648 140337659815808 model_lib_v2.py:707] Step 500 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.21349618,\n",
            " 'Loss/localization_loss': 0.17685318,\n",
            " 'Loss/regularization_loss': 0.15265045,\n",
            " 'Loss/total_loss': 0.5429998,\n",
            " 'learning_rate': 0.053333}\n",
            "I1022 21:12:38.318167 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.21349618,\n",
            " 'Loss/localization_loss': 0.17685318,\n",
            " 'Loss/regularization_loss': 0.15265045,\n",
            " 'Loss/total_loss': 0.5429998,\n",
            " 'learning_rate': 0.053333}\n",
            "INFO:tensorflow:Step 600 per-step time 0.410s\n",
            "I1022 21:13:19.236238 140337659815808 model_lib_v2.py:707] Step 600 per-step time 0.410s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.18175185,\n",
            " 'Loss/localization_loss': 0.15948592,\n",
            " 'Loss/regularization_loss': 0.15244533,\n",
            " 'Loss/total_loss': 0.4936831,\n",
            " 'learning_rate': 0.0586664}\n",
            "I1022 21:13:19.236653 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.18175185,\n",
            " 'Loss/localization_loss': 0.15948592,\n",
            " 'Loss/regularization_loss': 0.15244533,\n",
            " 'Loss/total_loss': 0.4936831,\n",
            " 'learning_rate': 0.0586664}\n",
            "INFO:tensorflow:Step 700 per-step time 0.407s\n",
            "I1022 21:13:59.996030 140337659815808 model_lib_v2.py:707] Step 700 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.20323353,\n",
            " 'Loss/localization_loss': 0.17854278,\n",
            " 'Loss/regularization_loss': 0.15224665,\n",
            " 'Loss/total_loss': 0.5340229,\n",
            " 'learning_rate': 0.0639998}\n",
            "I1022 21:13:59.996456 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.20323353,\n",
            " 'Loss/localization_loss': 0.17854278,\n",
            " 'Loss/regularization_loss': 0.15224665,\n",
            " 'Loss/total_loss': 0.5340229,\n",
            " 'learning_rate': 0.0639998}\n",
            "INFO:tensorflow:Step 800 per-step time 0.381s\n",
            "I1022 21:14:38.146955 140337659815808 model_lib_v2.py:707] Step 800 per-step time 0.381s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.15366387,\n",
            " 'Loss/localization_loss': 0.12571615,\n",
            " 'Loss/regularization_loss': 0.15217191,\n",
            " 'Loss/total_loss': 0.43155193,\n",
            " 'learning_rate': 0.069333196}\n",
            "I1022 21:14:38.153303 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.15366387,\n",
            " 'Loss/localization_loss': 0.12571615,\n",
            " 'Loss/regularization_loss': 0.15217191,\n",
            " 'Loss/total_loss': 0.43155193,\n",
            " 'learning_rate': 0.069333196}\n",
            "INFO:tensorflow:Step 900 per-step time 0.408s\n",
            "I1022 21:15:18.889987 140337659815808 model_lib_v2.py:707] Step 900 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.17613178,\n",
            " 'Loss/localization_loss': 0.12520184,\n",
            " 'Loss/regularization_loss': 0.15196458,\n",
            " 'Loss/total_loss': 0.45329818,\n",
            " 'learning_rate': 0.074666604}\n",
            "I1022 21:15:18.890625 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.17613178,\n",
            " 'Loss/localization_loss': 0.12520184,\n",
            " 'Loss/regularization_loss': 0.15196458,\n",
            " 'Loss/total_loss': 0.45329818,\n",
            " 'learning_rate': 0.074666604}\n",
            "INFO:tensorflow:Step 1000 per-step time 0.410s\n",
            "I1022 21:15:59.799823 140337659815808 model_lib_v2.py:707] Step 1000 per-step time 0.410s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.16989811,\n",
            " 'Loss/localization_loss': 0.10632176,\n",
            " 'Loss/regularization_loss': 0.15180533,\n",
            " 'Loss/total_loss': 0.4280252,\n",
            " 'learning_rate': 0.08}\n",
            "I1022 21:15:59.807455 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.16989811,\n",
            " 'Loss/localization_loss': 0.10632176,\n",
            " 'Loss/regularization_loss': 0.15180533,\n",
            " 'Loss/total_loss': 0.4280252,\n",
            " 'learning_rate': 0.08}\n",
            "INFO:tensorflow:Step 1100 per-step time 0.436s\n",
            "I1022 21:16:43.443626 140337659815808 model_lib_v2.py:707] Step 1100 per-step time 0.436s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.20914955,\n",
            " 'Loss/localization_loss': 0.16665898,\n",
            " 'Loss/regularization_loss': 0.151625,\n",
            " 'Loss/total_loss': 0.5274335,\n",
            " 'learning_rate': 0.07999918}\n",
            "I1022 21:16:43.444058 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.20914955,\n",
            " 'Loss/localization_loss': 0.16665898,\n",
            " 'Loss/regularization_loss': 0.151625,\n",
            " 'Loss/total_loss': 0.5274335,\n",
            " 'learning_rate': 0.07999918}\n",
            "INFO:tensorflow:Step 1200 per-step time 0.407s\n",
            "I1022 21:17:24.167601 140337659815808 model_lib_v2.py:707] Step 1200 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14377458,\n",
            " 'Loss/localization_loss': 0.10757668,\n",
            " 'Loss/regularization_loss': 0.15142883,\n",
            " 'Loss/total_loss': 0.40278012,\n",
            " 'learning_rate': 0.079996705}\n",
            "I1022 21:17:24.168715 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.14377458,\n",
            " 'Loss/localization_loss': 0.10757668,\n",
            " 'Loss/regularization_loss': 0.15142883,\n",
            " 'Loss/total_loss': 0.40278012,\n",
            " 'learning_rate': 0.079996705}\n",
            "INFO:tensorflow:Step 1300 per-step time 0.409s\n",
            "I1022 21:18:05.095405 140337659815808 model_lib_v2.py:707] Step 1300 per-step time 0.409s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13206345,\n",
            " 'Loss/localization_loss': 0.082470424,\n",
            " 'Loss/regularization_loss': 0.15109855,\n",
            " 'Loss/total_loss': 0.3656324,\n",
            " 'learning_rate': 0.0799926}\n",
            "I1022 21:18:05.095904 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.13206345,\n",
            " 'Loss/localization_loss': 0.082470424,\n",
            " 'Loss/regularization_loss': 0.15109855,\n",
            " 'Loss/total_loss': 0.3656324,\n",
            " 'learning_rate': 0.0799926}\n",
            "INFO:tensorflow:Step 1400 per-step time 0.405s\n",
            "I1022 21:18:45.642960 140337659815808 model_lib_v2.py:707] Step 1400 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.17473382,\n",
            " 'Loss/localization_loss': 0.112050146,\n",
            " 'Loss/regularization_loss': 0.1508025,\n",
            " 'Loss/total_loss': 0.43758646,\n",
            " 'learning_rate': 0.07998685}\n",
            "I1022 21:18:45.645955 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.17473382,\n",
            " 'Loss/localization_loss': 0.112050146,\n",
            " 'Loss/regularization_loss': 0.1508025,\n",
            " 'Loss/total_loss': 0.43758646,\n",
            " 'learning_rate': 0.07998685}\n",
            "INFO:tensorflow:Step 1500 per-step time 0.410s\n",
            "I1022 21:19:26.620595 140337659815808 model_lib_v2.py:707] Step 1500 per-step time 0.410s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.14442445,\n",
            " 'Loss/localization_loss': 0.10219397,\n",
            " 'Loss/regularization_loss': 0.15045998,\n",
            " 'Loss/total_loss': 0.3970784,\n",
            " 'learning_rate': 0.07997945}\n",
            "I1022 21:19:26.620956 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.14442445,\n",
            " 'Loss/localization_loss': 0.10219397,\n",
            " 'Loss/regularization_loss': 0.15045998,\n",
            " 'Loss/total_loss': 0.3970784,\n",
            " 'learning_rate': 0.07997945}\n",
            "INFO:tensorflow:Step 1600 per-step time 0.408s\n",
            "I1022 21:20:07.438050 140337659815808 model_lib_v2.py:707] Step 1600 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.160259,\n",
            " 'Loss/localization_loss': 0.10898468,\n",
            " 'Loss/regularization_loss': 0.15012866,\n",
            " 'Loss/total_loss': 0.41937232,\n",
            " 'learning_rate': 0.079970405}\n",
            "I1022 21:20:07.438490 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.160259,\n",
            " 'Loss/localization_loss': 0.10898468,\n",
            " 'Loss/regularization_loss': 0.15012866,\n",
            " 'Loss/total_loss': 0.41937232,\n",
            " 'learning_rate': 0.079970405}\n",
            "INFO:tensorflow:Step 1700 per-step time 0.405s\n",
            "I1022 21:20:47.883380 140337659815808 model_lib_v2.py:707] Step 1700 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.12218903,\n",
            " 'Loss/localization_loss': 0.09747182,\n",
            " 'Loss/regularization_loss': 0.14970624,\n",
            " 'Loss/total_loss': 0.3693671,\n",
            " 'learning_rate': 0.07995972}\n",
            "I1022 21:20:47.883786 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.12218903,\n",
            " 'Loss/localization_loss': 0.09747182,\n",
            " 'Loss/regularization_loss': 0.14970624,\n",
            " 'Loss/total_loss': 0.3693671,\n",
            " 'learning_rate': 0.07995972}\n",
            "INFO:tensorflow:Step 1800 per-step time 0.378s\n",
            "I1022 21:21:25.638285 140337659815808 model_lib_v2.py:707] Step 1800 per-step time 0.378s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1319394,\n",
            " 'Loss/localization_loss': 0.076536484,\n",
            " 'Loss/regularization_loss': 0.14924389,\n",
            " 'Loss/total_loss': 0.35771978,\n",
            " 'learning_rate': 0.0799474}\n",
            "I1022 21:21:25.638659 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.1319394,\n",
            " 'Loss/localization_loss': 0.076536484,\n",
            " 'Loss/regularization_loss': 0.14924389,\n",
            " 'Loss/total_loss': 0.35771978,\n",
            " 'learning_rate': 0.0799474}\n",
            "INFO:tensorflow:Step 1900 per-step time 0.410s\n",
            "I1022 21:22:06.638934 140337659815808 model_lib_v2.py:707] Step 1900 per-step time 0.410s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1632313,\n",
            " 'Loss/localization_loss': 0.11833915,\n",
            " 'Loss/regularization_loss': 0.14883292,\n",
            " 'Loss/total_loss': 0.43040335,\n",
            " 'learning_rate': 0.07993342}\n",
            "I1022 21:22:06.639318 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.1632313,\n",
            " 'Loss/localization_loss': 0.11833915,\n",
            " 'Loss/regularization_loss': 0.14883292,\n",
            " 'Loss/total_loss': 0.43040335,\n",
            " 'learning_rate': 0.07993342}\n",
            "INFO:tensorflow:Step 2000 per-step time 0.407s\n",
            "I1022 21:22:47.306531 140337659815808 model_lib_v2.py:707] Step 2000 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.116093144,\n",
            " 'Loss/localization_loss': 0.081926316,\n",
            " 'Loss/regularization_loss': 0.14840445,\n",
            " 'Loss/total_loss': 0.34642392,\n",
            " 'learning_rate': 0.07991781}\n",
            "I1022 21:22:47.307706 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.116093144,\n",
            " 'Loss/localization_loss': 0.081926316,\n",
            " 'Loss/regularization_loss': 0.14840445,\n",
            " 'Loss/total_loss': 0.34642392,\n",
            " 'learning_rate': 0.07991781}\n",
            "INFO:tensorflow:Step 2100 per-step time 0.441s\n",
            "I1022 21:23:31.431151 140337659815808 model_lib_v2.py:707] Step 2100 per-step time 0.441s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13754039,\n",
            " 'Loss/localization_loss': 0.0854225,\n",
            " 'Loss/regularization_loss': 0.1479697,\n",
            " 'Loss/total_loss': 0.37093258,\n",
            " 'learning_rate': 0.07990056}\n",
            "I1022 21:23:31.431570 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.13754039,\n",
            " 'Loss/localization_loss': 0.0854225,\n",
            " 'Loss/regularization_loss': 0.1479697,\n",
            " 'Loss/total_loss': 0.37093258,\n",
            " 'learning_rate': 0.07990056}\n",
            "INFO:tensorflow:Step 2200 per-step time 0.408s\n",
            "I1022 21:24:12.217897 140337659815808 model_lib_v2.py:707] Step 2200 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.12776835,\n",
            " 'Loss/localization_loss': 0.06362385,\n",
            " 'Loss/regularization_loss': 0.14753944,\n",
            " 'Loss/total_loss': 0.33893165,\n",
            " 'learning_rate': 0.07988167}\n",
            "I1022 21:24:12.218283 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.12776835,\n",
            " 'Loss/localization_loss': 0.06362385,\n",
            " 'Loss/regularization_loss': 0.14753944,\n",
            " 'Loss/total_loss': 0.33893165,\n",
            " 'learning_rate': 0.07988167}\n",
            "INFO:tensorflow:Step 2300 per-step time 0.411s\n",
            "I1022 21:24:53.236418 140337659815808 model_lib_v2.py:707] Step 2300 per-step time 0.411s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1292575,\n",
            " 'Loss/localization_loss': 0.06134486,\n",
            " 'Loss/regularization_loss': 0.14705883,\n",
            " 'Loss/total_loss': 0.3376612,\n",
            " 'learning_rate': 0.07986114}\n",
            "I1022 21:24:53.236808 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.1292575,\n",
            " 'Loss/localization_loss': 0.06134486,\n",
            " 'Loss/regularization_loss': 0.14705883,\n",
            " 'Loss/total_loss': 0.3376612,\n",
            " 'learning_rate': 0.07986114}\n",
            "INFO:tensorflow:Step 2400 per-step time 0.406s\n",
            "I1022 21:25:33.817875 140337659815808 model_lib_v2.py:707] Step 2400 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.13062598,\n",
            " 'Loss/localization_loss': 0.08905096,\n",
            " 'Loss/regularization_loss': 0.14660466,\n",
            " 'Loss/total_loss': 0.3662816,\n",
            " 'learning_rate': 0.07983897}\n",
            "I1022 21:25:33.818509 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.13062598,\n",
            " 'Loss/localization_loss': 0.08905096,\n",
            " 'Loss/regularization_loss': 0.14660466,\n",
            " 'Loss/total_loss': 0.3662816,\n",
            " 'learning_rate': 0.07983897}\n",
            "INFO:tensorflow:Step 2500 per-step time 0.408s\n",
            "I1022 21:26:14.651899 140337659815808 model_lib_v2.py:707] Step 2500 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11358967,\n",
            " 'Loss/localization_loss': 0.0771482,\n",
            " 'Loss/regularization_loss': 0.14616223,\n",
            " 'Loss/total_loss': 0.33690012,\n",
            " 'learning_rate': 0.079815164}\n",
            "I1022 21:26:14.652687 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11358967,\n",
            " 'Loss/localization_loss': 0.0771482,\n",
            " 'Loss/regularization_loss': 0.14616223,\n",
            " 'Loss/total_loss': 0.33690012,\n",
            " 'learning_rate': 0.079815164}\n",
            "INFO:tensorflow:Step 2600 per-step time 0.405s\n",
            "I1022 21:26:55.115467 140337659815808 model_lib_v2.py:707] Step 2600 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.112231925,\n",
            " 'Loss/localization_loss': 0.06138026,\n",
            " 'Loss/regularization_loss': 0.14568448,\n",
            " 'Loss/total_loss': 0.31929666,\n",
            " 'learning_rate': 0.07978972}\n",
            "I1022 21:26:55.115898 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.112231925,\n",
            " 'Loss/localization_loss': 0.06138026,\n",
            " 'Loss/regularization_loss': 0.14568448,\n",
            " 'Loss/total_loss': 0.31929666,\n",
            " 'learning_rate': 0.07978972}\n",
            "INFO:tensorflow:Step 2700 per-step time 0.405s\n",
            "I1022 21:27:35.704187 140337659815808 model_lib_v2.py:707] Step 2700 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.10249123,\n",
            " 'Loss/localization_loss': 0.06483819,\n",
            " 'Loss/regularization_loss': 0.14519529,\n",
            " 'Loss/total_loss': 0.31252474,\n",
            " 'learning_rate': 0.07976264}\n",
            "I1022 21:27:35.705292 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.10249123,\n",
            " 'Loss/localization_loss': 0.06483819,\n",
            " 'Loss/regularization_loss': 0.14519529,\n",
            " 'Loss/total_loss': 0.31252474,\n",
            " 'learning_rate': 0.07976264}\n",
            "INFO:tensorflow:Step 2800 per-step time 0.384s\n",
            "I1022 21:28:14.106186 140337659815808 model_lib_v2.py:707] Step 2800 per-step time 0.384s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.117509596,\n",
            " 'Loss/localization_loss': 0.08501454,\n",
            " 'Loss/regularization_loss': 0.14472294,\n",
            " 'Loss/total_loss': 0.34724706,\n",
            " 'learning_rate': 0.07973392}\n",
            "I1022 21:28:14.106624 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.117509596,\n",
            " 'Loss/localization_loss': 0.08501454,\n",
            " 'Loss/regularization_loss': 0.14472294,\n",
            " 'Loss/total_loss': 0.34724706,\n",
            " 'learning_rate': 0.07973392}\n",
            "INFO:tensorflow:Step 2900 per-step time 0.405s\n",
            "I1022 21:28:54.652678 140337659815808 model_lib_v2.py:707] Step 2900 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1025074,\n",
            " 'Loss/localization_loss': 0.0693785,\n",
            " 'Loss/regularization_loss': 0.14425501,\n",
            " 'Loss/total_loss': 0.31614092,\n",
            " 'learning_rate': 0.07970358}\n",
            "I1022 21:28:54.658919 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.1025074,\n",
            " 'Loss/localization_loss': 0.0693785,\n",
            " 'Loss/regularization_loss': 0.14425501,\n",
            " 'Loss/total_loss': 0.31614092,\n",
            " 'learning_rate': 0.07970358}\n",
            "INFO:tensorflow:Step 3000 per-step time 0.405s\n",
            "I1022 21:29:35.103178 140337659815808 model_lib_v2.py:707] Step 3000 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11035729,\n",
            " 'Loss/localization_loss': 0.082196966,\n",
            " 'Loss/regularization_loss': 0.14378433,\n",
            " 'Loss/total_loss': 0.33633858,\n",
            " 'learning_rate': 0.0796716}\n",
            "I1022 21:29:35.103672 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11035729,\n",
            " 'Loss/localization_loss': 0.082196966,\n",
            " 'Loss/regularization_loss': 0.14378433,\n",
            " 'Loss/total_loss': 0.33633858,\n",
            " 'learning_rate': 0.0796716}\n",
            "INFO:tensorflow:Step 3100 per-step time 0.433s\n",
            "I1022 21:30:18.426692 140337659815808 model_lib_v2.py:707] Step 3100 per-step time 0.433s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11176771,\n",
            " 'Loss/localization_loss': 0.07135472,\n",
            " 'Loss/regularization_loss': 0.14329097,\n",
            " 'Loss/total_loss': 0.3264134,\n",
            " 'learning_rate': 0.07963799}\n",
            "I1022 21:30:18.427089 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11176771,\n",
            " 'Loss/localization_loss': 0.07135472,\n",
            " 'Loss/regularization_loss': 0.14329097,\n",
            " 'Loss/total_loss': 0.3264134,\n",
            " 'learning_rate': 0.07963799}\n",
            "INFO:tensorflow:Step 3200 per-step time 0.408s\n",
            "I1022 21:30:59.177988 140337659815808 model_lib_v2.py:707] Step 3200 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.113946415,\n",
            " 'Loss/localization_loss': 0.07563555,\n",
            " 'Loss/regularization_loss': 0.14277509,\n",
            " 'Loss/total_loss': 0.33235705,\n",
            " 'learning_rate': 0.07960275}\n",
            "I1022 21:30:59.178372 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.113946415,\n",
            " 'Loss/localization_loss': 0.07563555,\n",
            " 'Loss/regularization_loss': 0.14277509,\n",
            " 'Loss/total_loss': 0.33235705,\n",
            " 'learning_rate': 0.07960275}\n",
            "INFO:tensorflow:Step 3300 per-step time 0.406s\n",
            "I1022 21:31:39.800379 140337659815808 model_lib_v2.py:707] Step 3300 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.104832284,\n",
            " 'Loss/localization_loss': 0.058335025,\n",
            " 'Loss/regularization_loss': 0.14225262,\n",
            " 'Loss/total_loss': 0.30541992,\n",
            " 'learning_rate': 0.07956588}\n",
            "I1022 21:31:39.800772 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.104832284,\n",
            " 'Loss/localization_loss': 0.058335025,\n",
            " 'Loss/regularization_loss': 0.14225262,\n",
            " 'Loss/total_loss': 0.30541992,\n",
            " 'learning_rate': 0.07956588}\n",
            "INFO:tensorflow:Step 3400 per-step time 0.409s\n",
            "I1022 21:32:20.755600 140337659815808 model_lib_v2.py:707] Step 3400 per-step time 0.409s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.1229767,\n",
            " 'Loss/localization_loss': 0.0879456,\n",
            " 'Loss/regularization_loss': 0.14175136,\n",
            " 'Loss/total_loss': 0.35267365,\n",
            " 'learning_rate': 0.079527386}\n",
            "I1022 21:32:20.757640 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.1229767,\n",
            " 'Loss/localization_loss': 0.0879456,\n",
            " 'Loss/regularization_loss': 0.14175136,\n",
            " 'Loss/total_loss': 0.35267365,\n",
            " 'learning_rate': 0.079527386}\n",
            "INFO:tensorflow:Step 3500 per-step time 0.407s\n",
            "I1022 21:33:01.443883 140337659815808 model_lib_v2.py:707] Step 3500 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.10039208,\n",
            " 'Loss/localization_loss': 0.055746693,\n",
            " 'Loss/regularization_loss': 0.14122407,\n",
            " 'Loss/total_loss': 0.29736286,\n",
            " 'learning_rate': 0.07948727}\n",
            "I1022 21:33:01.444636 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.10039208,\n",
            " 'Loss/localization_loss': 0.055746693,\n",
            " 'Loss/regularization_loss': 0.14122407,\n",
            " 'Loss/total_loss': 0.29736286,\n",
            " 'learning_rate': 0.07948727}\n",
            "INFO:tensorflow:Step 3600 per-step time 0.408s\n",
            "I1022 21:33:42.298536 140337659815808 model_lib_v2.py:707] Step 3600 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11228005,\n",
            " 'Loss/localization_loss': 0.08317403,\n",
            " 'Loss/regularization_loss': 0.14071591,\n",
            " 'Loss/total_loss': 0.33617,\n",
            " 'learning_rate': 0.079445526}\n",
            "I1022 21:33:42.298923 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11228005,\n",
            " 'Loss/localization_loss': 0.08317403,\n",
            " 'Loss/regularization_loss': 0.14071591,\n",
            " 'Loss/total_loss': 0.33617,\n",
            " 'learning_rate': 0.079445526}\n",
            "INFO:tensorflow:Step 3700 per-step time 0.407s\n",
            "I1022 21:34:22.922570 140337659815808 model_lib_v2.py:707] Step 3700 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.113545984,\n",
            " 'Loss/localization_loss': 0.06534774,\n",
            " 'Loss/regularization_loss': 0.14019175,\n",
            " 'Loss/total_loss': 0.31908548,\n",
            " 'learning_rate': 0.07940216}\n",
            "I1022 21:34:22.923099 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.113545984,\n",
            " 'Loss/localization_loss': 0.06534774,\n",
            " 'Loss/regularization_loss': 0.14019175,\n",
            " 'Loss/total_loss': 0.31908548,\n",
            " 'learning_rate': 0.07940216}\n",
            "INFO:tensorflow:Step 3800 per-step time 0.385s\n",
            "I1022 21:35:01.418894 140337659815808 model_lib_v2.py:707] Step 3800 per-step time 0.385s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.10753595,\n",
            " 'Loss/localization_loss': 0.061601445,\n",
            " 'Loss/regularization_loss': 0.13969228,\n",
            " 'Loss/total_loss': 0.30882967,\n",
            " 'learning_rate': 0.079357184}\n",
            "I1022 21:35:01.419574 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.10753595,\n",
            " 'Loss/localization_loss': 0.061601445,\n",
            " 'Loss/regularization_loss': 0.13969228,\n",
            " 'Loss/total_loss': 0.30882967,\n",
            " 'learning_rate': 0.079357184}\n",
            "INFO:tensorflow:Step 3900 per-step time 0.407s\n",
            "I1022 21:35:42.134116 140337659815808 model_lib_v2.py:707] Step 3900 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.10377822,\n",
            " 'Loss/localization_loss': 0.057533126,\n",
            " 'Loss/regularization_loss': 0.13919167,\n",
            " 'Loss/total_loss': 0.30050302,\n",
            " 'learning_rate': 0.07931058}\n",
            "I1022 21:35:42.134495 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.10377822,\n",
            " 'Loss/localization_loss': 0.057533126,\n",
            " 'Loss/regularization_loss': 0.13919167,\n",
            " 'Loss/total_loss': 0.30050302,\n",
            " 'learning_rate': 0.07931058}\n",
            "INFO:tensorflow:Step 4000 per-step time 0.406s\n",
            "I1022 21:36:22.746291 140337659815808 model_lib_v2.py:707] Step 4000 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.094758466,\n",
            " 'Loss/localization_loss': 0.048473302,\n",
            " 'Loss/regularization_loss': 0.13865364,\n",
            " 'Loss/total_loss': 0.2818854,\n",
            " 'learning_rate': 0.07926236}\n",
            "I1022 21:36:22.747913 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.094758466,\n",
            " 'Loss/localization_loss': 0.048473302,\n",
            " 'Loss/regularization_loss': 0.13865364,\n",
            " 'Loss/total_loss': 0.2818854,\n",
            " 'learning_rate': 0.07926236}\n",
            "INFO:tensorflow:Step 4100 per-step time 0.433s\n",
            "I1022 21:37:06.022300 140337659815808 model_lib_v2.py:707] Step 4100 per-step time 0.433s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08204553,\n",
            " 'Loss/localization_loss': 0.03587755,\n",
            " 'Loss/regularization_loss': 0.13809527,\n",
            " 'Loss/total_loss': 0.25601834,\n",
            " 'learning_rate': 0.07921253}\n",
            "I1022 21:37:06.022679 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.08204553,\n",
            " 'Loss/localization_loss': 0.03587755,\n",
            " 'Loss/regularization_loss': 0.13809527,\n",
            " 'Loss/total_loss': 0.25601834,\n",
            " 'learning_rate': 0.07921253}\n",
            "INFO:tensorflow:Step 4200 per-step time 0.407s\n",
            "I1022 21:37:46.692633 140337659815808 model_lib_v2.py:707] Step 4200 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.111748554,\n",
            " 'Loss/localization_loss': 0.056283534,\n",
            " 'Loss/regularization_loss': 0.13752787,\n",
            " 'Loss/total_loss': 0.30555993,\n",
            " 'learning_rate': 0.07916109}\n",
            "I1022 21:37:46.693008 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.111748554,\n",
            " 'Loss/localization_loss': 0.056283534,\n",
            " 'Loss/regularization_loss': 0.13752787,\n",
            " 'Loss/total_loss': 0.30555993,\n",
            " 'learning_rate': 0.07916109}\n",
            "INFO:tensorflow:Step 4300 per-step time 0.406s\n",
            "I1022 21:38:27.239101 140337659815808 model_lib_v2.py:707] Step 4300 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08983557,\n",
            " 'Loss/localization_loss': 0.05007102,\n",
            " 'Loss/regularization_loss': 0.13702463,\n",
            " 'Loss/total_loss': 0.27693123,\n",
            " 'learning_rate': 0.07910804}\n",
            "I1022 21:38:27.239601 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.08983557,\n",
            " 'Loss/localization_loss': 0.05007102,\n",
            " 'Loss/regularization_loss': 0.13702463,\n",
            " 'Loss/total_loss': 0.27693123,\n",
            " 'learning_rate': 0.07910804}\n",
            "INFO:tensorflow:Step 4400 per-step time 0.407s\n",
            "I1022 21:39:07.967340 140337659815808 model_lib_v2.py:707] Step 4400 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11208816,\n",
            " 'Loss/localization_loss': 0.07036286,\n",
            " 'Loss/regularization_loss': 0.1364848,\n",
            " 'Loss/total_loss': 0.3189358,\n",
            " 'learning_rate': 0.07905338}\n",
            "I1022 21:39:07.967709 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11208816,\n",
            " 'Loss/localization_loss': 0.07036286,\n",
            " 'Loss/regularization_loss': 0.1364848,\n",
            " 'Loss/total_loss': 0.3189358,\n",
            " 'learning_rate': 0.07905338}\n",
            "INFO:tensorflow:Step 4500 per-step time 0.406s\n",
            "I1022 21:39:48.521891 140337659815808 model_lib_v2.py:707] Step 4500 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09565532,\n",
            " 'Loss/localization_loss': 0.05265485,\n",
            " 'Loss/regularization_loss': 0.13594773,\n",
            " 'Loss/total_loss': 0.2842579,\n",
            " 'learning_rate': 0.07899711}\n",
            "I1022 21:39:48.522396 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09565532,\n",
            " 'Loss/localization_loss': 0.05265485,\n",
            " 'Loss/regularization_loss': 0.13594773,\n",
            " 'Loss/total_loss': 0.2842579,\n",
            " 'learning_rate': 0.07899711}\n",
            "INFO:tensorflow:Step 4600 per-step time 0.407s\n",
            "I1022 21:40:29.243315 140337659815808 model_lib_v2.py:707] Step 4600 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09708747,\n",
            " 'Loss/localization_loss': 0.049129017,\n",
            " 'Loss/regularization_loss': 0.13541973,\n",
            " 'Loss/total_loss': 0.2816362,\n",
            " 'learning_rate': 0.078939244}\n",
            "I1022 21:40:29.243877 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09708747,\n",
            " 'Loss/localization_loss': 0.049129017,\n",
            " 'Loss/regularization_loss': 0.13541973,\n",
            " 'Loss/total_loss': 0.2816362,\n",
            " 'learning_rate': 0.078939244}\n",
            "INFO:tensorflow:Step 4700 per-step time 0.406s\n",
            "I1022 21:41:09.887868 140337659815808 model_lib_v2.py:707] Step 4700 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09121985,\n",
            " 'Loss/localization_loss': 0.055086426,\n",
            " 'Loss/regularization_loss': 0.1348789,\n",
            " 'Loss/total_loss': 0.28118518,\n",
            " 'learning_rate': 0.07887978}\n",
            "I1022 21:41:09.888286 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09121985,\n",
            " 'Loss/localization_loss': 0.055086426,\n",
            " 'Loss/regularization_loss': 0.1348789,\n",
            " 'Loss/total_loss': 0.28118518,\n",
            " 'learning_rate': 0.07887978}\n",
            "INFO:tensorflow:Step 4800 per-step time 0.386s\n",
            "I1022 21:41:48.492486 140337659815808 model_lib_v2.py:707] Step 4800 per-step time 0.386s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09335097,\n",
            " 'Loss/localization_loss': 0.047853984,\n",
            " 'Loss/regularization_loss': 0.13438563,\n",
            " 'Loss/total_loss': 0.2755906,\n",
            " 'learning_rate': 0.07881871}\n",
            "I1022 21:41:48.492901 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09335097,\n",
            " 'Loss/localization_loss': 0.047853984,\n",
            " 'Loss/regularization_loss': 0.13438563,\n",
            " 'Loss/total_loss': 0.2755906,\n",
            " 'learning_rate': 0.07881871}\n",
            "INFO:tensorflow:Step 4900 per-step time 0.405s\n",
            "I1022 21:42:28.964266 140337659815808 model_lib_v2.py:707] Step 4900 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08401479,\n",
            " 'Loss/localization_loss': 0.047220908,\n",
            " 'Loss/regularization_loss': 0.13388388,\n",
            " 'Loss/total_loss': 0.26511955,\n",
            " 'learning_rate': 0.07875605}\n",
            "I1022 21:42:28.964658 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.08401479,\n",
            " 'Loss/localization_loss': 0.047220908,\n",
            " 'Loss/regularization_loss': 0.13388388,\n",
            " 'Loss/total_loss': 0.26511955,\n",
            " 'learning_rate': 0.07875605}\n",
            "INFO:tensorflow:Step 5000 per-step time 0.407s\n",
            "I1022 21:43:09.684147 140337659815808 model_lib_v2.py:707] Step 5000 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.091140956,\n",
            " 'Loss/localization_loss': 0.043627407,\n",
            " 'Loss/regularization_loss': 0.13336243,\n",
            " 'Loss/total_loss': 0.26813078,\n",
            " 'learning_rate': 0.078691795}\n",
            "I1022 21:43:09.684530 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.091140956,\n",
            " 'Loss/localization_loss': 0.043627407,\n",
            " 'Loss/regularization_loss': 0.13336243,\n",
            " 'Loss/total_loss': 0.26813078,\n",
            " 'learning_rate': 0.078691795}\n",
            "INFO:tensorflow:Step 5100 per-step time 0.434s\n",
            "I1022 21:43:53.045160 140337659815808 model_lib_v2.py:707] Step 5100 per-step time 0.434s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08490802,\n",
            " 'Loss/localization_loss': 0.042542066,\n",
            " 'Loss/regularization_loss': 0.13280885,\n",
            " 'Loss/total_loss': 0.26025894,\n",
            " 'learning_rate': 0.07862595}\n",
            "I1022 21:43:53.045527 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.08490802,\n",
            " 'Loss/localization_loss': 0.042542066,\n",
            " 'Loss/regularization_loss': 0.13280885,\n",
            " 'Loss/total_loss': 0.26025894,\n",
            " 'learning_rate': 0.07862595}\n",
            "INFO:tensorflow:Step 5200 per-step time 0.410s\n",
            "I1022 21:44:34.011247 140337659815808 model_lib_v2.py:707] Step 5200 per-step time 0.410s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.08672397,\n",
            " 'Loss/localization_loss': 0.049203593,\n",
            " 'Loss/regularization_loss': 0.13228336,\n",
            " 'Loss/total_loss': 0.26821092,\n",
            " 'learning_rate': 0.07855851}\n",
            "I1022 21:44:34.011677 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.08672397,\n",
            " 'Loss/localization_loss': 0.049203593,\n",
            " 'Loss/regularization_loss': 0.13228336,\n",
            " 'Loss/total_loss': 0.26821092,\n",
            " 'learning_rate': 0.07855851}\n",
            "INFO:tensorflow:Step 5300 per-step time 0.406s\n",
            "I1022 21:45:14.623460 140337659815808 model_lib_v2.py:707] Step 5300 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.11240289,\n",
            " 'Loss/localization_loss': 0.081341416,\n",
            " 'Loss/regularization_loss': 0.13174547,\n",
            " 'Loss/total_loss': 0.32548976,\n",
            " 'learning_rate': 0.07848949}\n",
            "I1022 21:45:14.623828 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.11240289,\n",
            " 'Loss/localization_loss': 0.081341416,\n",
            " 'Loss/regularization_loss': 0.13174547,\n",
            " 'Loss/total_loss': 0.32548976,\n",
            " 'learning_rate': 0.07848949}\n",
            "INFO:tensorflow:Step 5400 per-step time 0.408s\n",
            "I1022 21:45:55.322837 140337659815808 model_lib_v2.py:707] Step 5400 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.089475006,\n",
            " 'Loss/localization_loss': 0.052209985,\n",
            " 'Loss/regularization_loss': 0.13123423,\n",
            " 'Loss/total_loss': 0.27291924,\n",
            " 'learning_rate': 0.078418896}\n",
            "I1022 21:45:55.323229 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.089475006,\n",
            " 'Loss/localization_loss': 0.052209985,\n",
            " 'Loss/regularization_loss': 0.13123423,\n",
            " 'Loss/total_loss': 0.27291924,\n",
            " 'learning_rate': 0.078418896}\n",
            "INFO:tensorflow:Step 5500 per-step time 0.405s\n",
            "I1022 21:46:35.889468 140337659815808 model_lib_v2.py:707] Step 5500 per-step time 0.405s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09441081,\n",
            " 'Loss/localization_loss': 0.04678317,\n",
            " 'Loss/regularization_loss': 0.13071002,\n",
            " 'Loss/total_loss': 0.271904,\n",
            " 'learning_rate': 0.078346714}\n",
            "I1022 21:46:35.890534 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09441081,\n",
            " 'Loss/localization_loss': 0.04678317,\n",
            " 'Loss/regularization_loss': 0.13071002,\n",
            " 'Loss/total_loss': 0.271904,\n",
            " 'learning_rate': 0.078346714}\n",
            "INFO:tensorflow:Step 5600 per-step time 0.407s\n",
            "I1022 21:47:16.541415 140337659815808 model_lib_v2.py:707] Step 5600 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.085344404,\n",
            " 'Loss/localization_loss': 0.040558055,\n",
            " 'Loss/regularization_loss': 0.13019438,\n",
            " 'Loss/total_loss': 0.25609684,\n",
            " 'learning_rate': 0.07827295}\n",
            "I1022 21:47:16.541852 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.085344404,\n",
            " 'Loss/localization_loss': 0.040558055,\n",
            " 'Loss/regularization_loss': 0.13019438,\n",
            " 'Loss/total_loss': 0.25609684,\n",
            " 'learning_rate': 0.07827295}\n",
            "INFO:tensorflow:Step 5700 per-step time 0.407s\n",
            "I1022 21:47:57.271440 140337659815808 model_lib_v2.py:707] Step 5700 per-step time 0.407s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.076455794,\n",
            " 'Loss/localization_loss': 0.035294387,\n",
            " 'Loss/regularization_loss': 0.12966213,\n",
            " 'Loss/total_loss': 0.24141231,\n",
            " 'learning_rate': 0.07819763}\n",
            "I1022 21:47:57.271790 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.076455794,\n",
            " 'Loss/localization_loss': 0.035294387,\n",
            " 'Loss/regularization_loss': 0.12966213,\n",
            " 'Loss/total_loss': 0.24141231,\n",
            " 'learning_rate': 0.07819763}\n",
            "INFO:tensorflow:Step 5800 per-step time 0.382s\n",
            "I1022 21:48:35.484551 140337659815808 model_lib_v2.py:707] Step 5800 per-step time 0.382s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.07155121,\n",
            " 'Loss/localization_loss': 0.024973683,\n",
            " 'Loss/regularization_loss': 0.12917733,\n",
            " 'Loss/total_loss': 0.22570223,\n",
            " 'learning_rate': 0.07812072}\n",
            "I1022 21:48:35.490657 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.07155121,\n",
            " 'Loss/localization_loss': 0.024973683,\n",
            " 'Loss/regularization_loss': 0.12917733,\n",
            " 'Loss/total_loss': 0.22570223,\n",
            " 'learning_rate': 0.07812072}\n",
            "INFO:tensorflow:Step 5900 per-step time 0.408s\n",
            "I1022 21:49:16.253922 140337659815808 model_lib_v2.py:707] Step 5900 per-step time 0.408s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09881789,\n",
            " 'Loss/localization_loss': 0.051508818,\n",
            " 'Loss/regularization_loss': 0.12866373,\n",
            " 'Loss/total_loss': 0.27899045,\n",
            " 'learning_rate': 0.078042254}\n",
            "I1022 21:49:16.254292 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09881789,\n",
            " 'Loss/localization_loss': 0.051508818,\n",
            " 'Loss/regularization_loss': 0.12866373,\n",
            " 'Loss/total_loss': 0.27899045,\n",
            " 'learning_rate': 0.078042254}\n",
            "INFO:tensorflow:Step 6000 per-step time 0.406s\n",
            "I1022 21:49:56.897090 140337659815808 model_lib_v2.py:707] Step 6000 per-step time 0.406s\n",
            "INFO:tensorflow:{'Loss/classification_loss': 0.09206762,\n",
            " 'Loss/localization_loss': 0.044202022,\n",
            " 'Loss/regularization_loss': 0.12813327,\n",
            " 'Loss/total_loss': 0.26440293,\n",
            " 'learning_rate': 0.07796223}\n",
            "I1022 21:49:56.897454 140337659815808 model_lib_v2.py:708] {'Loss/classification_loss': 0.09206762,\n",
            " 'Loss/localization_loss': 0.044202022,\n",
            " 'Loss/regularization_loss': 0.12813327,\n",
            " 'Loss/total_loss': 0.26440293,\n",
            " 'learning_rate': 0.07796223}\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "TRAINING_SCRIPT = os.path.join(paths[\"obj_detection_api\"],\"model_main_tf2.py\")\n",
        "\n",
        "trainCmd = \"python {} --model_dir={} --pipeline_config_path={} --num_train_steps={}\".format(TRAINING_SCRIPT, os.path.join(paths[\"custom_models\"], CUSTOM_MODEL_NAME), paths[\"custom_model_config\"],NOF_STEPS)\n",
        "#evalCmd = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths[\"custom_model_dir\"], paths[\"custom_model_config\"], paths[\"custom_model_dir\"])\n",
        "\n",
        "\n",
        "!{trainCmd}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48UbAlBYABdm"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znEgFtf8FBFb"
      },
      "source": [
        "Create evaluation files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVU-sCcKAEsl"
      },
      "outputs": [],
      "source": [
        "command = \"python {} --model_dir={} --pipeline_config_path={} --checkpoint_dir={}\".format(TRAINING_SCRIPT, paths[\"custom_model_dir\"], paths[\"custom_model_config\"], paths[\"custom_model_dir\"])\n",
        "print(command)\n",
        "!{command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-7T7lCXFDkB"
      },
      "source": [
        "load tensorboard extension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dtrLXvLkFGDa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d7d0742-a657-4ebf-a398-54a7d91216c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        }
      ],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wSwNl4KC5ad"
      },
      "source": [
        "Evaluate training loss with Tensorbord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTVjx1iNCoI5"
      },
      "outputs": [],
      "source": [
        "trainingLogDir = os.path.join(paths[\"custom_model_dir\"],\"train\")\n",
        "%tensorboard --logdir {trainingLogDir}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_C4MS6sJFI8L"
      },
      "source": [
        "Evaluate Test results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Q9fomE9FmSk"
      },
      "outputs": [],
      "source": [
        "evalLogDir = paths[\"custom_model_dir\"]\n",
        "%tensorboard --logdir {evalLogDir} --samples_per_plugin=images=100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8vcvgfDGqRY"
      },
      "source": [
        "# Export and save the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfhZPheA7J8N"
      },
      "source": [
        "Export the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CcCmhsRwX8t9"
      },
      "outputs": [],
      "source": [
        "import shutil as sh\n",
        "\n",
        "# Copy exporter script\n",
        "expScriptLocalPath = os.path.join(paths[\"training\"],\"exporter_main_v2.py\")\n",
        "if not os.path.exists(expScriptLocalPath):\n",
        "  sh.copyfile(os.path.join(paths['obj_detection_api'], \"exporter_main_v2.py\"), expScriptLocalPath)\n",
        "\n",
        "# run the script\n",
        "!python {expScriptLocalPath} --input_type image_tensor --pipeline_config_path {paths['custom_model_config']} --trained_checkpoint_dir {paths['custom_model_dir']} --output_directory {os.path.join(paths[\"model_export_dir\"], CUSTOM_MODEL_NAME)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e04RputVaKyP"
      },
      "source": [
        "Push exported model to git branch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FKElQdVvaNEQ"
      },
      "outputs": [],
      "source": [
        "%cd {paths[\"github_repo\"]}\n",
        "!git fetch\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AupXj-N3aSYR"
      },
      "outputs": [],
      "source": [
        "!git pull\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsxN-SIMabF9"
      },
      "outputs": [],
      "source": [
        "!git add {paths[\"model_export_dir\"]}\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzIjIfBea7KQ"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"linus.heinzelmann@uni-ulm.de\"\n",
        "!git config --global user.name \"lheinzel\"\n",
        "!git commit -m \"model trained on colab\"\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eS6NmonQlI_z"
      },
      "outputs": [],
      "source": [
        "!git remote"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exRRBU4VlMn_"
      },
      "outputs": [],
      "source": [
        "!git push origin {GIT_BRANCH}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HneSlBWyh7Rh"
      },
      "source": [
        "# Save the current training state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KF8S_t6iFJV",
        "outputId": "7c2c128b-446a-486c-c52d-6105d41a3255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/UAVHRBuildingDetection\n",
            "On branch SSD_MobNet_320x320_Augmenter\n",
            "Your branch is up to date with 'origin/SSD_MobNet_320x320_Augmenter'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/checkpoint\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.index\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mTensorflow/scripts/preprocessing/__pycache__/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/test.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/train.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/images/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/pretrained_model/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084211.6f95152c6f26.1567.0.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084636.6f95152c6f26.1567.1.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084947.6f95152c6f26.1567.2.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085259.6f95152c6f26.1567.3.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085573.6f95152c6f26.1567.4.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085888.6f95152c6f26.1567.5.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086202.6f95152c6f26.1567.6.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086516.6f95152c6f26.1567.7.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086827.6f95152c6f26.1567.8.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087140.6f95152c6f26.1567.9.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087452.6f95152c6f26.1567.10.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/train/events.out.tfevents.1663084221.6f95152c6f26.1566.0.v2\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "%cd {paths[\"github_repo\"]}\n",
        "!git fetch\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4YaB3LOiFvU",
        "outputId": "d40e9806-1083-4f10-c00c-cfcdafa609c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n",
            "On branch SSD_MobNet_320x320_Augmenter\n",
            "Your branch is up to date with 'origin/SSD_MobNet_320x320_Augmenter'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add/rm <file>...\" to update what will be committed)\n",
            "  (use \"git checkout -- <file>...\" to discard changes in working directory)\n",
            "\n",
            "\t\u001b[31mmodified:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/checkpoint\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.index\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.index\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mTensorflow/scripts/preprocessing/__pycache__/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/test.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/train.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/images/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/pretrained_model/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.data-00000-of-00001\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.index\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084211.6f95152c6f26.1567.0.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084636.6f95152c6f26.1567.1.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084947.6f95152c6f26.1567.2.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085259.6f95152c6f26.1567.3.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085573.6f95152c6f26.1567.4.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085888.6f95152c6f26.1567.5.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086202.6f95152c6f26.1567.6.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086516.6f95152c6f26.1567.7.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086827.6f95152c6f26.1567.8.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087140.6f95152c6f26.1567.9.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087452.6f95152c6f26.1567.10.v2\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/train/events.out.tfevents.1663084221.6f95152c6f26.1566.0.v2\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git pull\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxMFPySLiPeJ",
        "outputId": "0b13c932-b206-40c1-aa85-eda27087ebb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "On branch SSD_MobNet_320x320_Augmenter\n",
            "Your branch is up to date with 'origin/SSD_MobNet_320x320_Augmenter'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/checkpoint\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.index\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.data-00000-of-00001\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.index\u001b[m\n",
            "\t\u001b[32mdeleted:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.index\u001b[m\n",
            "\t\u001b[32mrenamed:    Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.data-00000-of-00001 -> Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084211.6f95152c6f26.1567.0.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084636.6f95152c6f26.1567.1.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084947.6f95152c6f26.1567.2.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085259.6f95152c6f26.1567.3.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085573.6f95152c6f26.1567.4.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085888.6f95152c6f26.1567.5.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086202.6f95152c6f26.1567.6.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086516.6f95152c6f26.1567.7.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086827.6f95152c6f26.1567.8.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087140.6f95152c6f26.1567.9.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087452.6f95152c6f26.1567.10.v2\u001b[m\n",
            "\t\u001b[32mnew file:   Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/train/events.out.tfevents.1663084221.6f95152c6f26.1566.0.v2\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mTensorflow/scripts/preprocessing/__pycache__/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/test.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/train.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/images/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/pretrained_model/\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!git add {paths[\"custom_models\"]}\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfIE7gV2imR_",
        "outputId": "199c8731-a2b3-47b2-9895-355dfa734534"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SSD_MobNet_320x320_Augmenter cc69b44] model trained on colab\n",
            " 34 files changed, 16 insertions(+), 16 deletions(-)\n",
            " rewrite Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/checkpoint (93%)\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-10.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-11.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-9.data-00000-of-00001 => ckpt-15.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-15.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-5.data-00000-of-00001 => ckpt-16.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-16.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-10.data-00000-of-00001 => ckpt-17.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-17.index\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.data-00000-of-00001\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-18.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-7.data-00000-of-00001 => ckpt-19.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-19.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-11.data-00000-of-00001 => ckpt-20.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-20.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-8.data-00000-of-00001 => ckpt-21.data-00000-of-00001} (68%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-21.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-5.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-6.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-7.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-8.index\n",
            " delete mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/ckpt-9.index\n",
            " rename Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/{ckpt-6.data-00000-of-00001 => eval/events.out.tfevents.1663084211.6f95152c6f26.1567.0.v2} (55%)\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084636.6f95152c6f26.1567.1.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663084947.6f95152c6f26.1567.2.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085259.6f95152c6f26.1567.3.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085573.6f95152c6f26.1567.4.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663085888.6f95152c6f26.1567.5.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086202.6f95152c6f26.1567.6.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086516.6f95152c6f26.1567.7.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663086827.6f95152c6f26.1567.8.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087140.6f95152c6f26.1567.9.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/eval/events.out.tfevents.1663087452.6f95152c6f26.1567.10.v2\n",
            " create mode 100644 Tensorflow/workspace/training_SSD-MobnetV2_320x320_MoreAugments/models/HRDetection_MobNetV2/train/events.out.tfevents.1663084221.6f95152c6f26.1566.0.v2\n",
            "On branch SSD_MobNet_320x320_Augmenter\n",
            "Your branch is ahead of 'origin/SSD_MobNet_320x320_Augmenter' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\n",
            "\t\u001b[31mTensorflow/scripts/preprocessing/__pycache__/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/test.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/annotations/train.record\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/images/\u001b[m\n",
            "\t\u001b[31mTensorflow/workspace/pretrained_model/\u001b[m\n",
            "\n",
            "nothing added to commit but untracked files present (use \"git add\" to track)\n"
          ]
        }
      ],
      "source": [
        "!git config --global user.email \"linus.heinzelmann@uni-ulm.de\"\n",
        "!git config --global user.name \"lheinzel\"\n",
        "!git commit -m \"model trained on colab\"\n",
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skLMvbofin-S",
        "outputId": "3e4e11df-45a8-4c7f-f2e9-660fd4c2503f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counting objects: 36, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects: 100% (35/35), done.\n",
            "Writing objects: 100% (36/36), 292.90 MiB | 7.54 MiB/s, done.\n",
            "Total 36 (delta 10), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (10/10), completed with 3 local objects.\u001b[K\n",
            "To https://github.com/lheinzel/UAVHRBuildingDetection.git\n",
            "   849462f..cc69b44  SSD_MobNet_320x320_Augmenter -> SSD_MobNet_320x320_Augmenter\n"
          ]
        }
      ],
      "source": [
        "!git push origin {GIT_BRANCH}"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "HR_BuildingDetector_Training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "9b770e305961dc0abb9a6ba30fcb22311445fc021163a190ce871a4494dafc24"
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}